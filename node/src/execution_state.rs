// Copyright (c) 2021, Facebook, Inc. and its affiliates
// Copyright (c) 2022, Mysten Labs, Inc.
// SPDX-License-Identifier: Apache-2.0

// Include protobuf-generated code
mod comm {
    #![allow(clippy::derive_partial_eq_without_eq)]
    include!(concat!(env!("OUT_DIR"), "/comm.rs"));
}

// Include transaction protobuf ƒë·ªÉ parse v√† t√≠nh hash ƒë√∫ng c√°ch
mod transaction {
    #![allow(clippy::derive_partial_eq_without_eq)]
    include!(concat!(env!("OUT_DIR"), "/transaction.rs"));
}

use async_trait::async_trait;
use bincode;
use bytes::Bytes;
use consensus::ConsensusOutput;
use executor::{ExecutionIndices, ExecutionState};
use prost::Message;
use sha3::{Digest, Keccak256};
use hex;
use types::{BatchDigest, ConsensusStore};
use storage::CertificateStore;
use std::{
    collections::{HashMap, HashSet},
    sync::Arc,
    time::{Duration, Instant},
    path::PathBuf,
    fs,
};
use tokio::{
    io::AsyncWriteExt,
    net::UnixStream,
    sync::Mutex,
    time::{sleep, Duration as TokioDuration},
};
use tracing::{debug, error, info, warn};
use serde::{Serialize, Deserialize};

/// Macro cho UDS debug logs - ch·ªâ compile trong debug mode
/// Gi√∫p gi·∫£m overhead trong production builds
/// Note: Trong release builds, c√°c logs n√†y s·∫Ω kh√¥ng ƒë∆∞·ª£c compile (no-op)
#[cfg(debug_assertions)]
macro_rules! uds_debug {
    ($($arg:tt)*) => {
        debug!($($arg)*);
    };
}

#[cfg(not(debug_assertions))]
macro_rules! uds_debug {
    ($($arg:tt)*) => {
        // No-op in release builds
    };
}

/// Danh s√°ch c√°c transaction hash c·∫ßn trace ƒë·ªÉ debug
/// Th√™m hash v√†o ƒë√¢y ƒë·ªÉ trace giao d·ªãch c·ª• th·ªÉ
const TRACE_TX_HASHES: &[&str] = &[
    "3a5c3e3b26972417ab9735cd248919038511b9244296f401321a979ea0473a37",
    "f10856e44dd7522b1177183d90a3e078cacee7b6a79a96019f67f7ff50544cc9",
    "f6ff74b6bdcfd6d5714f7dd07de5b85f8e493e9ff2198e7704d850ae4d55a47c",
    "68fa4f572cd0de5bc2566479a854569d5cb0ec493abb45594a969c372a2f4575",
    "db1ba03c94f7c92b397930881d1dbfedd97528eb962dd9cd2c94650ae9dde5ba",
    "f7a91ea44ac8d31bcaeb95616f3902e6e7db9dab28764a17ae6dda2f1219de97", // Hash g·ªëc
    "c5d2460186f7233c927e7db2dcc703c0e500b653ca82273b7bfad8045d85a470", // Hash sai (n·∫øu xu·∫•t hi·ªán)
    "8e8aca3c6c64b719f1defb6a7c0f2aea04efd79749d1581c078d41a347501221", // Hash g·ªëc c·∫ßn trace
    "f73489a241e58510a040aa37b890b1e92bc3962c108158766218c2ded7acd755", // Hash g·ªëc c·∫ßn trace
    "940502a6459a1871f2189f330e277526b2066aaf53ff7db566b47d962cee73db", // Hash g·ªëc c·∫ßn trace
];

/// Check xem transaction hash c√≥ c·∫ßn trace kh√¥ng
fn should_trace_tx(tx_hash_hex: &str) -> bool {
    TRACE_TX_HASHES.contains(&tx_hash_hex)
}

/// T√≠nh hash c·ªßa transaction t·ª´ Transaction object
/// OPTIMIZED: S·ª≠ d·ª•ng shared logic t·ª´ worker::transaction_logger ƒë·ªÉ tr√°nh code duplication
/// 
/// Th·ªëng nh·∫•t v·ªõi Go: T·∫°o TransactionHashData t·ª´ Transaction, encode th√†nh protobuf, r·ªìi t√≠nh Keccak256 hash
/// ƒê·∫£m b·∫£o hash kh·ªõp gi·ªØa Go v√† Rust v√¨ c·∫£ hai ƒë·ªÅu t√≠nh t·ª´ TransactionHashData (protobuf encoded)
/// 
/// CRITICAL: N·∫øu kh√¥ng t√≠nh ƒë∆∞·ª£c hash ‚Üí PANIC (d·ª´ng ch∆∞∆°ng tr√¨nh), KH√îNG d√πng fallback hash
/// Calculate transaction hash from protobuf Transaction object
/// CRITICAL: H√†m n√†y CH·ªà ƒë·ªçc d·ªØ li·ªáu t·ª´ protobuf object ƒë·ªÉ t√≠nh hash
/// - KH√îNG serialize l·∫°i transaction bytes
/// - KH√îNG ·∫£nh h∆∞·ªüng ƒë·∫øn transaction bytes g·ªëc
/// - Hash ƒë∆∞·ª£c t√≠nh t·ª´ TransactionHashData protobuf encoding (theo chu·∫©n Go implementation)
/// - Bytes ƒë∆∞·ª£c g·ª≠i qua UDS ph·∫£i l√† bytes g·ªëc (kh√¥ng serialize l·∫°i)
/// 
/// NOTE: Function n√†y gi·ªØ l·∫°i ƒë·ªÉ maintain compatibility v·ªõi node's transaction type
/// Logic t√≠nh hash gi·ªëng h·ªát worker::transaction_logger::calculate_transaction_hash
/// nh∆∞ng s·ª≠ d·ª•ng node's transaction::Transaction type
fn calculate_transaction_hash_from_proto(tx: &transaction::Transaction) -> Vec<u8> {
    // OPTIMIZATION: Logic gi·ªëng h·ªát worker::transaction_logger::calculate_transaction_hash
    // Gi·ªØ l·∫°i function n√†y v√¨ node v√† worker c√≥ th·ªÉ c√≥ different protobuf-generated types
    // nh∆∞ng logic t√≠nh hash ho√†n to√†n gi·ªëng nhau
    
    // T·∫°o TransactionHashData t·ª´ Transaction (ch·ªâ ƒë·ªçc fields, kh√¥ng serialize transaction)
    let hash_data = transaction::TransactionHashData {
        from_address: tx.from_address.clone(),
        to_address: tx.to_address.clone(),
        amount: tx.amount.clone(),
        max_gas: tx.max_gas,
        max_gas_price: tx.max_gas_price,
        max_time_use: tx.max_time_use,
        data: tx.data.clone(),
        r#type: tx.r#type,
        last_device_key: tx.last_device_key.clone(),
        new_device_key: tx.new_device_key.clone(),
        nonce: tx.nonce.clone(),
        chain_id: tx.chain_id,
        r: tx.r.clone(),
        s: tx.s.clone(),
        v: tx.v.clone(),
        gas_tip_cap: tx.gas_tip_cap.clone(),
        gas_fee_cap: tx.gas_fee_cap.clone(),
        access_list: tx
            .access_list
            .iter()
            .map(|at| transaction::AccessTuple {
                address: at.address.clone(),
                storage_keys: at.storage_keys.clone(),
            })
            .collect(),
    };

    // Encode hash_data th√†nh bytes
    let mut buf = Vec::new();
    hash_data.encode(&mut buf).expect("CRITICAL: Failed to encode TransactionHashData - cannot continue without correct hash");

    // T√≠nh Keccak256 hash
    let hash = Keccak256::digest(&buf);
    hash.to_vec()
}

/// Parse transaction t·ª´ raw bytes
/// CRITICAL: 1 batch ch·ª©a m·ªôt m·∫£ng giao d·ªãch
/// - Transaction bytes c√≥ th·ªÉ l√† protobuf `Transactions` (ch·ª©a nhi·ªÅu Transaction)
/// - Ho·∫∑c protobuf `Transaction` (single transaction)
/// - Ho·∫∑c raw bytes kh√¥ng ph·∫£i protobuf (fallback: t√≠nh hash t·ª´ raw bytes)
/// - Transaction bytes c√≥ TH·ªÇ c√≥ 8-byte length prefix (nh∆∞ trong batch) ho·∫∑c KH√îNG (nh∆∞ t·ª´ client)
/// 
/// Returns: Vec<(tx_hash_hex, tx_hash, Option<tx_proto>, raw_bytes)> cho T·∫§T C·∫¢ transactions
/// - tx_proto = Some() n·∫øu parse ƒë∆∞·ª£c protobuf, None n·∫øu raw bytes
/// - raw_bytes = transaction bytes g·ªëc ƒë·ªÉ l∆∞u v√†o block
fn parse_transactions_from_bytes(transaction_bytes: &[u8]) -> Vec<(String, Vec<u8>, Option<transaction::Transaction>, Vec<u8>)> {
    // CRITICAL: Th·ª≠ decode TR·ª∞C TI·∫æP tr∆∞·ªõc (gi·ªëng worker.rs - kh√¥ng c√≥ prefix)
    // N·∫øu kh√¥ng ƒë∆∞·ª£c, m·ªõi th·ª≠ strip 8-byte prefix (gi·ªëng batch_maker.rs - c√≥ prefix)
    
    // Helper function ƒë·ªÉ extract transaction bytes g·ªëc t·ª´ Transactions wrapper
    // CRITICAL: Ph·∫£i extract bytes g·ªëc t·ª´ wrapper, KH√îNG serialize l·∫°i
    // - Serialize l·∫°i c√≥ th·ªÉ t·∫°o ra bytes kh√°c ‚Üí hash kh√°c ‚Üí Go kh√¥ng nh·∫≠n ƒë∆∞·ª£c ƒë√∫ng
    // - CH·ªà C√ì 1 C√ÅCH: Extract bytes g·ªëc t·ª´ wrapper (wire format parsing)
    fn extract_transaction_bytes_from_wrapper(
        wrapper_bytes: &[u8],
        tx_index: usize,
    ) -> Option<Vec<u8>> {
        let mut offset = 0;
        let mut current_index = 0;
        
        // Parse Transactions wrapper: l·∫∑p qua c√°c field v·ªõi tag 0x0a (repeated Transaction)
        while offset < wrapper_bytes.len() {
            if offset >= wrapper_bytes.len() {
                break;
            }
            
            let field_tag = wrapper_bytes[offset];
            offset += 1;
            
            // Field tag format: (field_number << 3) | wire_type
            // Field number 1 for `repeated Transaction transactions = 1;` is 0x0a (1 << 3 | 2)
            if field_tag != 0x0a {
                // Kh√¥ng ph·∫£i field transactions, skip field n√†y d·ª±a tr√™n wire type
                let wire_type = field_tag & 0x07;
                match wire_type {
                    0 => {
                        // Varint: skip ƒë·∫øn khi g·∫∑p byte kh√¥ng c√≥ bit 0x80
                        while offset < wrapper_bytes.len() {
                            if (wrapper_bytes[offset] & 0x80) == 0 {
                                offset += 1;
                                break;
                            }
                            offset += 1;
                        }
                    }
                    1 => {
                        // Fixed64: skip 8 bytes
                        if offset + 8 <= wrapper_bytes.len() {
                            offset += 8;
                        } else {
                            return None;
                        }
                    }
                    2 => {
                        // Length-delimited: read varint length v√† skip
                        let mut length = 0u32;
                        let mut shift = 0;
                        loop {
                            if offset >= wrapper_bytes.len() {
                                return None;
                            }
                            let byte = wrapper_bytes[offset];
                            offset += 1;
                            length |= ((byte & 0x7F) as u32) << shift;
                            if (byte & 0x80) == 0 {
                                break;
                            }
                            shift += 7;
                            if shift >= 32 {
                                return None;
                            }
                        }
                        if offset + length as usize > wrapper_bytes.len() {
                            return None;
                        }
                        offset += length as usize;
                    }
                    5 => {
                        // Fixed32: skip 4 bytes
                        if offset + 4 <= wrapper_bytes.len() {
                            offset += 4;
                        } else {
                            return None;
                        }
                    }
                    _ => {
                        return None;
                    }
                }
                continue;
            }
            
            // ƒê√¢y l√† field transactions (0x0a), ƒë·ªçc varint length
            let mut length = 0u32;
            let mut shift = 0;
            loop {
                if offset >= wrapper_bytes.len() {
                    return None;
                }
                let byte = wrapper_bytes[offset];
                offset += 1;
                length |= ((byte & 0x7F) as u32) << shift;
                if (byte & 0x80) == 0 {
                    break;
                }
                shift += 7;
                if shift >= 32 {
                    return None;
                }
            }
            
            // Validate length
            if offset + length as usize > wrapper_bytes.len() {
                return None;
            }
            
            // Extract to√†n b·ªô Transaction bytes
            if current_index == tx_index {
                let extracted_bytes = wrapper_bytes[offset..offset + length as usize].to_vec();
                return Some(extracted_bytes);
            }
            
            // Skip transaction n√†y v√† ti·∫øp t·ª•c t√¨m transaction ti·∫øp theo
            offset += length as usize;
            current_index += 1;
        }
        
        None
    }
    
    // Helper function ƒë·ªÉ x·ª≠ l√Ω k·∫øt qu·∫£ parse th√†nh c√¥ng
    // CRITICAL: CH·ªà C√ì 1 C√ÅCH DUY NH·∫§T - D√πng bytes g·ªëc t·ª´ Go
    // - Hash ƒë∆∞·ª£c t√≠nh t·ª´ protobuf object (TransactionHashData)
    // - Bytes g·ª≠i ƒëi ph·∫£i l√† bytes g·ªëc t·ª´ Go (KH√îNG serialize l·∫°i)
    // - N·∫øu wrapper ch·ªâ c√≥ 1 transaction v√† original_bytes c√≥ th·ªÉ parse nh∆∞ Transaction tr·ª±c ti·∫øp
    //   ‚Üí d√πng original_bytes tr·ª±c ti·∫øp (kh√¥ng c·∫ßn extract)
    // - N·∫øu wrapper c√≥ nhi·ªÅu transactions ‚Üí extract t·ª´ng transaction t·ª´ wrapper
    fn process_decoded_transactions(
        txs: transaction::Transactions,
        original_bytes: &[u8],
    ) -> Vec<(String, Vec<u8>, Option<transaction::Transaction>, Vec<u8>)> {
        if txs.transactions.is_empty() {
            error!("‚ùå [UDS] CRITICAL: Transactions decoded but contains no transactions. Cannot process empty wrapper.");
            panic!("CRITICAL: Transactions wrapper is empty - cannot proceed");
        }
        
        info!("üîç [UDS] Processing Transactions wrapper: TxCount={}, WrapperLen={} bytes", 
            txs.transactions.len(), original_bytes.len());
        
        // CRITICAL: LU√îN extract transaction bytes t·ª´ wrapper (KH√îNG d√πng original_bytes tr·ª±c ti·∫øp)
        // - original_bytes l√† wrapper bytes (Transactions protobuf), KH√îNG ph·∫£i transaction bytes b√™n trong
        // - Worker t√≠nh hash t·ª´ transaction object b√™n trong wrapper ‚Üí hash kh√°c v·ªõi wrapper bytes
        // - Primary ph·∫£i extract transaction bytes b√™n trong wrapper ƒë·ªÉ hash kh·ªõp v·ªõi worker
        // - N·∫øu wrapper ch·ªâ c√≥ 1 transaction, v·∫´n ph·∫£i extract ƒë·ªÉ ƒë·∫£m b·∫£o hash kh·ªõp v·ªõi worker
        let mut results = Vec::new();
        for (tx_idx, tx) in txs.transactions.iter().enumerate() {
            // CRITICAL: T√≠nh hash t·ª´ protobuf object (TransactionHashData)
            let tx_hash = calculate_transaction_hash_from_proto(tx);
            let tx_hash_hex = hex::encode(&tx_hash);
            
            // CRITICAL: LU√îN serialize t·ª´ protobuf object ƒë·ªÉ ƒë·∫£m b·∫£o bytes ƒë√∫ng format
            // V·∫§N ƒê·ªÄ: extract_transaction_bytes_from_wrapper c√≥ th·ªÉ extract kh√¥ng ƒë√∫ng ho·∫∑c extract wrapper bytes
            // GI·∫¢I PH√ÅP: Serialize t·ª´ protobuf object (ƒë√£ parse) ƒë·ªÉ ƒë·∫£m b·∫£o bytes ƒë√∫ng format
            // - Serialize t·ª´ protobuf object s·∫Ω t·∫°o ra bytes ƒë√∫ng format m√† Go c√≥ th·ªÉ parse
            // - Hash v·∫´n ƒë∆∞·ª£c t√≠nh t·ª´ TransactionHashData (kh√¥ng ·∫£nh h∆∞·ªüng)
            // - Bytes serialize t·ª´ protobuf object s·∫Ω l√† transaction bytes g·ªëc, kh√¥ng ph·∫£i wrapper
            let mut tx_bytes = Vec::new();
            tx.encode(&mut tx_bytes).expect("CRITICAL: Failed to encode Transaction");
            
            // VALIDATION: ƒê·∫£m b·∫£o serialized bytes c√≥ th·ªÉ parse l·∫°i v√† hash kh·ªõp
            // NOTE: Kh√¥ng check xem bytes c√≥ th·ªÉ parse nh∆∞ Transactions wrapper v√¨:
            // - Transaction bytes h·ª£p l·ªá c√≥ th·ªÉ v√¥ t√¨nh parse ƒë∆∞·ª£c nh∆∞ wrapper (do protobuf wire format)
            // - Ch·ªâ c·∫ßn check xem bytes c√≥ th·ªÉ parse l·∫°i nh∆∞ Transaction v√† hash kh·ªõp l√† ƒë·ªß
            match transaction::Transaction::decode(tx_bytes.as_slice()) {
                Ok(parsed_tx) => {
                    let parsed_hash = calculate_transaction_hash_from_proto(&parsed_tx);
                    let parsed_hash_hex = hex::encode(&parsed_hash);
                    if parsed_hash_hex != tx_hash_hex {
                        error!("‚ùå [UDS] CRITICAL: Hash mismatch after serialize! Expected: {}, Got: {}, TxBytesLen: {}", 
                            tx_hash_hex, parsed_hash_hex, tx_bytes.len());
                        
                        // CRITICAL: Log hex khi hash mismatch
                        if should_trace_tx(&tx_hash_hex) {
                            let tx_bytes_hex = hex::encode(&tx_bytes);
                            error!("‚ùå [UDS] TRACE: Serialized bytes hex when hash mismatch for {}: {} (full: {})", 
                                tx_hash_hex, tx_bytes_hex, tx_bytes_hex);
                        }
                        
                        panic!("CRITICAL: Serialized transaction hash mismatch - cannot proceed");
                    }
                }
                Err(e) => {
                    error!("‚ùå [UDS] CRITICAL: Serialized bytes cannot be parsed! Error: {:?}, TxBytesLen: {}", 
                        e, tx_bytes.len());
                    
                    // CRITICAL: Log hex khi parse failed
                    if should_trace_tx(&tx_hash_hex) {
                        let tx_bytes_hex = hex::encode(&tx_bytes);
                        error!("‚ùå [UDS] TRACE: Serialized bytes hex when parse failed for {}: {} (full: {})", 
                            tx_hash_hex, tx_bytes_hex, tx_bytes_hex);
                    }
                    
                    panic!("CRITICAL: Serialized transaction cannot be parsed - cannot proceed");
                }
            }
            
            info!("‚úÖ [UDS] Prepared transaction[{}] bytes. TxHash: {}, BytesLen: {} bytes, WrapperLen: {} bytes", 
                tx_idx, tx_hash_hex, tx_bytes.len(), original_bytes.len());
            
            // CRITICAL: Log hex c·ªßa tx_bytes ƒë·ªÉ trace (ch·ªâ cho transaction ƒë∆∞·ª£c trace)
            if should_trace_tx(&tx_hash_hex) {
                let tx_bytes_hex = hex::encode(&tx_bytes);
                info!("üîç [UDS] TRACE: Transaction bytes hex for {}: {} (first 100 chars: {})", 
                    tx_hash_hex, 
                    if tx_bytes_hex.len() > 200 { format!("{}...", &tx_bytes_hex[..200]) } else { tx_bytes_hex.clone() },
                    if tx_bytes_hex.len() > 100 { &tx_bytes_hex[..100] } else { &tx_bytes_hex });
            }
            
            // VALIDATION: ƒê·∫£m b·∫£o tx_bytes c√≥ th·ªÉ parse l·∫°i ƒë∆∞·ª£c nh∆∞ Transaction v√† hash kh·ªõp
            match transaction::Transaction::decode(tx_bytes.as_slice()) {
                Ok(parsed_tx) => {
                    let parsed_hash = calculate_transaction_hash_from_proto(&parsed_tx);
                    let parsed_hash_hex = hex::encode(&parsed_hash);
                    if parsed_hash_hex != tx_hash_hex {
                        error!("‚ùå [UDS] CRITICAL: Hash mismatch after prepare! Original hash: {}, Parsed hash: {}, TxBytesLen: {}", 
                            tx_hash_hex, parsed_hash_hex, tx_bytes.len());
                        
                        // CRITICAL: Log hex khi hash mismatch
                        if should_trace_tx(&tx_hash_hex) {
                            let tx_bytes_hex = hex::encode(&tx_bytes);
                            error!("‚ùå [UDS] TRACE: Transaction bytes hex when hash mismatch for {}: {} (full: {})", 
                                tx_hash_hex, tx_bytes_hex, tx_bytes_hex);
                        }
                        
                        panic!("CRITICAL: Transaction bytes hash mismatch - cannot proceed");
                    } else {
                        uds_debug!("‚úÖ [UDS] Transaction bytes validated: TxHash={}, BytesLen={}", tx_hash_hex, tx_bytes.len());
                    }
                }
                Err(e) => {
                    error!("‚ùå [UDS] CRITICAL: Transaction bytes cannot be parsed! TxHash: {}, BytesLen: {}, Error: {:?}", 
                        tx_hash_hex, tx_bytes.len(), e);
                    
                    // CRITICAL: Log hex khi parse failed
                    if should_trace_tx(&tx_hash_hex) {
                        let tx_bytes_hex = hex::encode(&tx_bytes);
                        error!("‚ùå [UDS] TRACE: Transaction bytes hex when parse failed for {}: {} (full: {})", 
                            tx_hash_hex, tx_bytes_hex, tx_bytes_hex);
                    }
                    
                    panic!("CRITICAL: Transaction bytes cannot be parsed - cannot proceed");
                }
            }
            
            results.push((tx_hash_hex, tx_hash, Some(tx.clone()), tx_bytes));
        }
        results
    }
    
    fn process_decoded_transaction(
        tx: transaction::Transaction,
        original_bytes: &[u8],
    ) -> Vec<(String, Vec<u8>, Option<transaction::Transaction>, Vec<u8>)> {
        let tx_hash = calculate_transaction_hash_from_proto(&tx);
        let tx_hash_hex = hex::encode(&tx_hash);
        
        // CRITICAL: LU√îN serialize t·ª´ protobuf object ƒë·ªÉ ƒë·∫£m b·∫£o bytes ƒë√∫ng format
        // V·∫§N ƒê·ªÄ: original_bytes c√≥ th·ªÉ l√† wrapper bytes (Transactions protobuf) thay v√¨ transaction bytes
        // GI·∫¢I PH√ÅP: Serialize t·ª´ protobuf object (ƒë√£ parse) ƒë·ªÉ ƒë·∫£m b·∫£o bytes ƒë√∫ng format
        // - Serialize t·ª´ protobuf object s·∫Ω t·∫°o ra bytes ƒë√∫ng format m√† Go c√≥ th·ªÉ parse
        // - Hash v·∫´n ƒë∆∞·ª£c t√≠nh t·ª´ TransactionHashData (kh√¥ng ·∫£nh h∆∞·ªüng)
        // - Bytes serialize t·ª´ protobuf object s·∫Ω l√† transaction bytes g·ªëc, kh√¥ng ph·∫£i wrapper
        let mut tx_bytes = Vec::new();
        tx.encode(&mut tx_bytes).expect("CRITICAL: Failed to encode Transaction");
        
        // VALIDATION: ƒê·∫£m b·∫£o serialized bytes c√≥ th·ªÉ parse l·∫°i v√† hash kh·ªõp
        // NOTE: Kh√¥ng check xem bytes c√≥ th·ªÉ parse nh∆∞ Transactions wrapper v√¨:
        // - Transaction bytes h·ª£p l·ªá c√≥ th·ªÉ v√¥ t√¨nh parse ƒë∆∞·ª£c nh∆∞ wrapper (do protobuf wire format)
        // - Ch·ªâ c·∫ßn check xem bytes c√≥ th·ªÉ parse l·∫°i nh∆∞ Transaction v√† hash kh·ªõp l√† ƒë·ªß
        match transaction::Transaction::decode(tx_bytes.as_slice()) {
            Ok(parsed_tx) => {
                let parsed_hash = calculate_transaction_hash_from_proto(&parsed_tx);
                let parsed_hash_hex = hex::encode(&parsed_hash);
                if parsed_hash_hex != tx_hash_hex {
                    error!("‚ùå [UDS] CRITICAL: Hash mismatch after serialize! Expected: {}, Got: {}, TxBytesLen: {}", 
                        tx_hash_hex, parsed_hash_hex, tx_bytes.len());
                    
                    // CRITICAL: Log hex khi hash mismatch
                    if should_trace_tx(&tx_hash_hex) {
                        let tx_bytes_hex = hex::encode(&tx_bytes);
                        error!("‚ùå [UDS] TRACE: Transaction bytes hex when hash mismatch for {}: {} (full: {})", 
                            tx_hash_hex, tx_bytes_hex, tx_bytes_hex);
                    }
                    
                    panic!("CRITICAL: Serialized transaction hash mismatch - cannot proceed");
                } else {
                    uds_debug!("‚úÖ [UDS] Serialized transaction bytes validated: TxHash={}, BytesLen={}", tx_hash_hex, tx_bytes.len());
                }
            }
            Err(e) => {
                error!("‚ùå [UDS] CRITICAL: Serialized bytes cannot be parsed! Error: {:?}, TxBytesLen: {}", 
                    e, tx_bytes.len());
                
                // CRITICAL: Log hex khi parse failed
                if should_trace_tx(&tx_hash_hex) {
                    let tx_bytes_hex = hex::encode(&tx_bytes);
                    error!("‚ùå [UDS] TRACE: Transaction bytes hex when parse failed for {}: {} (full: {})", 
                        tx_hash_hex, tx_bytes_hex, tx_bytes_hex);
                }
                
                panic!("CRITICAL: Serialized transaction cannot be parsed - cannot proceed");
            }
        }
        
        info!("‚úÖ [UDS] Prepared single transaction bytes. TxHash: {}, BytesLen: {} bytes, OriginalBytesLen: {} bytes", 
            tx_hash_hex, tx_bytes.len(), original_bytes.len());
        
        // CRITICAL: Log hex c·ªßa tx_bytes ƒë·ªÉ trace (ch·ªâ cho transaction ƒë∆∞·ª£c trace)
        if should_trace_tx(&tx_hash_hex) {
            let tx_bytes_hex = hex::encode(&tx_bytes);
            info!("üîç [UDS] TRACE: Single transaction bytes hex for {}: {} (first 100 chars: {})", 
                tx_hash_hex, 
                if tx_bytes_hex.len() > 200 { format!("{}...", &tx_bytes_hex[..200]) } else { tx_bytes_hex.clone() },
                if tx_bytes_hex.len() > 100 { &tx_bytes_hex[..100] } else { &tx_bytes_hex });
        }
        
        vec![(tx_hash_hex, tx_hash, Some(tx), tx_bytes)]
    }
    
    // Th·ª≠ 1: Parse TR·ª∞C TI·∫æP nh∆∞ Transactions (gi·ªëng worker.rs - kh√¥ng c√≥ prefix)
    // N·∫øu parse th√†nh c√¥ng nh∆∞ Transactions wrapper ‚Üí extract t·ª´ wrapper
    if let Ok(txs) = transaction::Transactions::decode(transaction_bytes) {
        info!("‚úÖ [UDS] Parsed as Transactions wrapper: TxCount={}, BytesLen={}", 
            txs.transactions.len(), transaction_bytes.len());
        return process_decoded_transactions(txs, transaction_bytes);
    }
    
    // Th·ª≠ 2: Parse TR·ª∞C TI·∫æP nh∆∞ single Transaction (gi·ªëng worker.rs - kh√¥ng c√≥ prefix)
    // N·∫øu parse th√†nh c√¥ng nh∆∞ single Transaction ‚Üí d√πng bytes tr·ª±c ti·∫øp (KH√îNG extract)
    if let Ok(tx) = transaction::Transaction::decode(transaction_bytes) {
        info!("‚úÖ [UDS] Parsed as single Transaction: BytesLen={}", transaction_bytes.len());
        return process_decoded_transaction(tx, transaction_bytes);
    }
    
    // Th·ª≠ 3: Strip 8-byte prefix v√† parse nh∆∞ Transactions (gi·ªëng batch_maker.rs - c√≥ prefix)
    const LENGTH_PREFIX_SIZE: usize = 8;
    if transaction_bytes.len() > LENGTH_PREFIX_SIZE {
        let payload = &transaction_bytes[LENGTH_PREFIX_SIZE..];
        
        if let Ok(txs) = transaction::Transactions::decode(payload) {
            // CRITICAL: Extract t·ª´ payload (ƒë√£ strip 8-byte prefix), kh√¥ng ph·∫£i t·ª´ transaction_bytes (c√≥ prefix)
            return process_decoded_transactions(txs, payload);
        }
        
        if let Ok(tx) = transaction::Transaction::decode(payload) {
            return process_decoded_transaction(tx, transaction_bytes);
        }
    }
    
    // CRITICAL: Kh√¥ng parse ƒë∆∞·ª£c protobuf ‚Üí PANIC (kh√¥ng c√≥ fallback)
    // - Ch·ªâ c√≥ 1 c√°ch serialize duy nh·∫•t: t·ª´ protobuf object
    // - N·∫øu kh√¥ng parse ƒë∆∞·ª£c ‚Üí kh√¥ng th·ªÉ serialize ‚Üí PANIC
    error!(
        "‚ùå [UDS] CRITICAL: Cannot parse transaction as Transactions or Transaction (tried both with and without 8-byte prefix). \
        Transaction bytes length: {}, FirstBytes: {:02x?}",
        transaction_bytes.len(),
        if transaction_bytes.len() >= 20 { &transaction_bytes[..20] } else { transaction_bytes }
    );
    panic!(
        "CRITICAL: Cannot parse transaction bytes as protobuf. BytesLen: {} bytes. \
        Cannot proceed without correct protobuf parsing. \
        CH·ªà C√ì 1 C√ÅCH SERIALIZE DUY NH·∫§T - KH√îNG C√ì FALLBACK.",
        transaction_bytes.len()
    );
}

/// Parse transaction t·ª´ raw bytes v√† t√≠nh hash cho transaction ƒë·∫ßu ti√™n (backward compatibility)
/// DEPRECATED: D√πng parse_transactions_from_bytes() ƒë·ªÉ l·∫•y T·∫§T C·∫¢ transactions
/// H√†m n√†y ch·ªâ tr·∫£ v·ªÅ hash c·ªßa transaction ƒë·∫ßu ti√™n (d√πng cho logging)
fn parse_transaction_and_calculate_hash(transaction_bytes: &[u8]) -> Option<(String, Vec<u8>)> {
    let results = parse_transactions_from_bytes(transaction_bytes);
    // L·∫•y transaction ƒë·∫ßu ti√™n ƒë·ªÉ backward compatibility
    results.first().map(|(hash_hex, hash, _, _)| (hash_hex.clone(), hash.clone()))
}

/// Block size: G·ªôp BLOCK_SIZE consensus_index th√†nh 1 block
const BLOCK_SIZE: u64 = 10;

/// GC depth: S·ªë blocks gi·ªØ l·∫°i trong processed_batch_digests ƒë·ªÉ cleanup entries c≈©
/// Gi·ªØ l·∫°i entries v·ªõi consensus_index >= current_consensus_index - GC_DEPTH * BLOCK_SIZE
/// V√≠ d·ª•: GC_DEPTH=100, BLOCK_SIZE=10 ‚Üí gi·ªØ l·∫°i 1000 consensus_index g·∫ßn nh·∫•t
const GC_DEPTH: u64 = 100;

/// Timeout ƒë·ªÉ xem batch l√† "missed" (b·ªã b·ªè r∆°i) sau khi ƒë∆∞·ª£c commit
/// Sau th·ªùi gian n√†y, batch ƒë∆∞·ª£c xem l√† missed v√† c·∫ßn retry
/// Default: 5 gi√¢y
const MISSED_BATCH_TIMEOUT_SECS: u64 = 5;

/// Max retry attempts cho m·ªôt batch b·ªã missed
/// Sau s·ªë l·∫ßn retry n√†y, batch s·∫Ω b·ªã b·ªè qua
/// Default: 3
const MAX_MISSED_BATCH_RETRIES: u32 = 3;

/// Th√¥ng tin batch b·ªã missed (ch∆∞a ƒë∆∞·ª£c processed sau khi commit)
#[derive(Clone, Debug)]
struct MissedBatchInfo {
    /// Th·ªùi gian batch ƒë∆∞·ª£c commit
    commit_time: Instant,
    /// Consensus index c·ªßa batch
    consensus_index: u64,
    /// Round c·ªßa batch
    round: u64,
    /// Block height c·ªßa batch
    block_height: u64,
    /// S·ªë l·∫ßn ƒë√£ retry
    retry_count: u32,
    /// Th·ªùi gian retry cu·ªëi c√πng
    last_retry_time: Instant,
}

/// Transaction entry v·ªõi consensus_index ƒë·ªÉ ƒë·∫£m b·∫£o deterministic ordering
#[derive(Clone)]
struct TransactionEntry {
    consensus_index: u64,
    transaction: comm::Transaction,
    /// Hash ƒë√£ t√≠nh s·∫µn ƒë·ªÉ kh√¥ng ph·∫£i t√≠nh l·∫°i khi finalize
    tx_hash_hex: String,
    /// Batch digest ƒë·ªÉ check duplicate khi retry block
    /// None n·∫øu kh√¥ng c√≥ batch_digest (empty block ho·∫∑c batch kh√¥ng c√≥ trong certificate payload)
    batch_digest: Option<BatchDigest>,
}

struct BlockBuilder {
    epoch: u64,
    /// Block height = consensus_index / BLOCK_SIZE
    height: u64,
    /// Transactions v·ªõi consensus_index ƒë·ªÉ sort deterministic
    transaction_entries: Vec<TransactionEntry>,
    /// Track transaction hashes trong block n√†y ƒë·ªÉ tr√°nh duplicate
    transaction_hashes: HashSet<Vec<u8>>,
}

/// Execution state persisted to disk for crash recovery
#[derive(Serialize, Deserialize, Clone, Debug, Default)]
struct PersistedExecutionState {
    last_consensus_index: u64,
    last_sent_height: Option<u64>,
}

/// Execution state that sends blocks progressively via UDS (no batching, no size limit)
pub struct UdsExecutionState {
    /// UDS socket path
    socket_path: String,
    /// Current epoch (assumed constant for now)
    epoch: u64,
    /// Current block being built, keyed by block height (consensus_index / BLOCK_SIZE)
    current_block: Arc<Mutex<Option<BlockBuilder>>>,
    /// Last sent height (to detect gaps and send empty blocks)
    /// None = ch∆∞a g·ª≠i block n√†o, Some(h) = ƒë√£ g·ª≠i ƒë·∫øn block h
    last_sent_height: Arc<Mutex<Option<u64>>>,
    /// Last consensus index processed
    last_consensus_index: Arc<Mutex<u64>>,
    /// UDS stream (lazy connection)
    stream: Arc<Mutex<Option<UnixStream>>>,
    /// Late certificates buffer: L∆∞u th√¥ng tin certificate ƒë·∫øn mu·ªôn (sau khi block ƒë√£ g·ª≠i)
    /// Format: (block_height, consensus_index, round, has_transaction)
    late_certificates: Arc<Mutex<Vec<(u64, u64, u64, bool)>>>,
    /// Max retries cho block sending
    max_send_retries: u32,
    /// Retry delay base (milliseconds)
    retry_delay_base_ms: u64,
    /// Track batch_digest ƒë√£ x·ª≠ l√Ω v·ªõi consensus_index t∆∞∆°ng ·ª©ng
    /// CRITICAL: Prevent duplicate execution c·ªßa batch khi ƒë∆∞·ª£c re-included v√† commit l·∫°i
    /// Format: HashMap<BatchDigest, u64> - map t·ª´ batch_digest ƒë·∫øn consensus_index ƒë√£ x·ª≠ l√Ω
    /// PRODUCTION-SAFE: ƒê·∫£m b·∫£o batch ch·ªâ ƒë∆∞·ª£c x·ª≠ l√Ω m·ªôt l·∫ßn duy nh·∫•t cho m·ªói consensus_index
    /// FORK-SAFE: T·∫•t c·∫£ nodes track c√πng batches ‚Üí c√πng quy·∫øt ƒë·ªãnh skip ‚Üí fork-safe
    processed_batch_digests: Arc<Mutex<HashMap<BatchDigest, u64>>>,
    /// Track batches ƒë√£ commit nh∆∞ng ch∆∞a ƒë∆∞·ª£c processed (c√≥ th·ªÉ b·ªã missed)
    /// Format: HashMap<BatchDigest, MissedBatchInfo>
    /// M·ª•c ƒë√≠ch: Ph√°t hi·ªán v√† retry batches b·ªã b·ªè r∆°i m·ªôt c√°ch th√¥ng minh (kh√¥ng retry li√™n t·ª•c)
    missed_batches: Arc<Mutex<HashMap<BatchDigest, MissedBatchInfo>>>,
    /// Timeout ƒë·ªÉ xem batch l√† missed (milliseconds)
    missed_batch_timeout_ms: u64,
    /// Max retry attempts cho missed batch
    max_missed_batch_retries: u32,
    /// Track c√°c batch ƒë√£ log warning v·ªÅ duplicate ƒë·ªÉ tr√°nh log l·∫∑p l·∫°i (gi·ªõi h·∫°n 1000 entries)
    /// Format: HashSet<BatchDigest> - ch·ªâ log l·∫ßn ƒë·∫ßu ti√™n cho m·ªói batch
    logged_duplicate_batches: Arc<Mutex<HashSet<BatchDigest>>>,
    /// Path to execution state file for persistence
    execution_state_path: Option<PathBuf>,
    /// Consensus store reference for catch-up mechanism
    consensus_store: Option<Arc<ConsensusStore>>,
    /// Certificate store reference for recovery
    certificate_store: Option<CertificateStore>,
    /// Threshold for triggering catch-up (number of certificates lag)
    catch_up_threshold: u64,
    /// Interval for checking execution lag
    catch_up_check_interval: Duration,
    /// Counter for persistence (persist every N certificates)
    persistence_counter: Arc<Mutex<u64>>,
}

impl BlockBuilder {
    /// Finalize block: sort transactions theo consensus_index v√† convert sang CommittedBlock
    /// ƒê·∫£m b·∫£o deterministic ordering - t·∫•t c·∫£ nodes t·∫°o c√πng block t·ª´ c√πng certificates
    /// 
    /// CRITICAL: Th·ªëng nh·∫•t format v·ªõi Go - g·ª≠i Transactions wrapper (gi·ªëng Go g·ª≠i)
    /// - G·ªôp t·∫•t c·∫£ transactions trong block th√†nh m·ªôt Transactions wrapper
    /// - G·ª≠i wrapper bytes trong digest c·ªßa transaction ƒë·∫ßu ti√™n
    /// - C√°c transaction kh√°c c√≥ digest r·ªóng (ho·∫∑c kh√¥ng g·ª≠i)
    /// 
    /// Returns: (CommittedBlock, transaction_hashes_map, batch_digests) - map t·ª´ digest bytes ‚Üí tx_hash_hex v√† danh s√°ch batch_digests
    fn finalize(&self) -> (comm::CommittedBlock, HashMap<Vec<u8>, String>, Vec<Option<BatchDigest>>) {
        // CRITICAL: Sort theo consensus_index ƒë·ªÉ ƒë·∫£m b·∫£o deterministic ordering
        // FORK-SAFE: 
        // - Primary sort: consensus_index (deterministic t·ª´ consensus)
        // - Secondary sort: tx_hash_hex (deterministic string comparison)
        // - T·∫•t c·∫£ nodes nh·∫≠n c√πng consensus_index sequence ‚Üí c√πng sort order ‚Üí c√πng block content ‚Üí fork-safe
        // - Protobuf repeated field gi·ªØ nguy√™n order ‚Üí wrapper bytes deterministic ‚Üí fork-safe
        let mut sorted_entries = self.transaction_entries.clone();
        sorted_entries.sort_by(|a, b| {
            // Primary sort: consensus_index
            match a.consensus_index.cmp(&b.consensus_index) {
                std::cmp::Ordering::Equal => {
                    // Secondary sort: tx_hash_hex (deterministic string comparison)
                    // ƒê·∫£m b·∫£o transactions c√πng consensus_index c√≥ c√πng order tr√™n t·∫•t c·∫£ nodes
                    a.tx_hash_hex.cmp(&b.tx_hash_hex)
                }
                other => other,
            }
        });
        
        // CRITICAL: Th·ªëng nh·∫•t format v·ªõi Go - g·ª≠i Transactions wrapper
        // G·ªôp t·∫•t c·∫£ transactions trong block th√†nh m·ªôt Transactions wrapper
        let (transactions, tx_hash_map): (Vec<comm::Transaction>, HashMap<Vec<u8>, String>) = if sorted_entries.is_empty() {
            (Vec::new(), HashMap::new())
        } else {
            // Parse t·∫•t c·∫£ transaction bytes t·ª´ digest
            let mut tx_protos = Vec::new();
            for entry in &sorted_entries {
                match transaction::Transaction::decode(entry.transaction.digest.as_ref() as &[u8]) {
                    Ok(tx) => tx_protos.push(tx),
                    Err(e) => {
                        error!("‚ùå [UDS] CRITICAL: Cannot parse transaction bytes in finalize! TxHash: {}, Error: {:?}", 
                            entry.tx_hash_hex, e);
                        panic!("CRITICAL: Cannot parse transaction bytes in finalize!");
                    }
                }
            }
            
            // T·∫°o Transactions wrapper
            let transactions_wrapper = transaction::Transactions {
                transactions: tx_protos,
            };
            
            // Serialize Transactions wrapper
            let mut wrapper_bytes = Vec::new();
            transactions_wrapper.encode(&mut wrapper_bytes)
                .expect("CRITICAL: Failed to encode Transactions wrapper");
            
            // VALIDATION: ƒê·∫£m b·∫£o wrapper bytes c√≥ th·ªÉ parse l·∫°i
            match transaction::Transactions::decode(wrapper_bytes.as_slice()) {
                Ok(parsed_wrapper) => {
                    if parsed_wrapper.transactions.len() != sorted_entries.len() {
                        error!("‚ùå [UDS] CRITICAL: Wrapper transaction count mismatch! Expected: {}, Parsed: {}", 
                            sorted_entries.len(), parsed_wrapper.transactions.len());
                        panic!("CRITICAL: Wrapper transaction count mismatch!");
                    }
                    uds_debug!("‚úÖ [UDS] Wrapper validation: {} transactions in wrapper", parsed_wrapper.transactions.len());
                    
                    // VALIDATION: ƒê·∫£m b·∫£o hash c·ªßa t·ª´ng transaction trong wrapper kh·ªõp v·ªõi hash ƒë√£ l∆∞u
                    for (idx, tx) in parsed_wrapper.transactions.iter().enumerate() {
                        let wrapper_tx_hash = calculate_transaction_hash_from_proto(tx);
                        let wrapper_tx_hash_hex = hex::encode(&wrapper_tx_hash);
                        let expected_hash = &sorted_entries[idx].tx_hash_hex;
                        
                        if wrapper_tx_hash_hex != *expected_hash {
                            error!("‚ùå [UDS] CRITICAL: Wrapper transaction hash mismatch! Block {} Tx[{}]: Expected={}, Wrapper={}", 
                                self.height, idx, expected_hash, wrapper_tx_hash_hex);
                            panic!("CRITICAL: Wrapper transaction hash mismatch!");
                        }
                    }
                }
                Err(e) => {
                    error!("‚ùå [UDS] CRITICAL: Cannot parse wrapper bytes in finalize! Error: {:?}", e);
                    panic!("CRITICAL: Cannot parse wrapper bytes in finalize!");
                }
            }
            
            // T·∫°o CommittedBlock v·ªõi wrapper bytes trong digest c·ªßa transaction ƒë·∫ßu ti√™n
            // C√°c transaction kh√°c c√≥ digest r·ªóng (ho·∫∑c kh√¥ng g·ª≠i)
            let wrapper_digest = Bytes::from(wrapper_bytes.clone());
            let first_worker_id = sorted_entries[0].transaction.worker_id;
            
            // T·∫°o map t·ª´ wrapper digest bytes ‚Üí tx_hash_hex (ch·ªâ cho transaction ƒë·∫ßu ti√™n, v√¨ wrapper ch·ª©a t·∫•t c·∫£)
            // CRITICAL: Wrapper bytes ch·ª©a t·∫•t c·∫£ transactions, n√™n ch·ªâ c·∫ßn map wrapper bytes ‚Üí hash c·ªßa transaction ƒë·∫ßu ti√™n
            // Go s·∫Ω parse wrapper v√† t√≠nh hash cho t·ª´ng transaction
            let mut tx_hash_map = HashMap::new();
            let first_tx_hash = sorted_entries[0].tx_hash_hex.clone();
            tx_hash_map.insert(wrapper_bytes, first_tx_hash);
            
            (
                vec![comm::Transaction {
                    digest: wrapper_digest,
                    worker_id: first_worker_id,
                }],
                tx_hash_map,
            )
        };
        
        // Log t·∫•t c·∫£ transaction hashes khi finalize block (d√πng hash ƒë√£ l∆∞u s·∫µn)
        if !sorted_entries.is_empty() {
            info!("üì¶ [UDS] Finalizing block {} with {} transactions:", self.height, sorted_entries.len());
            for (idx, entry) in sorted_entries.iter().enumerate() {
                info!("  üìã [UDS] Block {} Tx[{}] in final block: TxHash={}, WorkerId={}, ConsensusIndex={}", 
                    self.height, idx, entry.tx_hash_hex, entry.transaction.worker_id, entry.consensus_index);
                
                // CRITICAL: Log ƒë·ªÉ trace giao d·ªãch trong final block
                if should_trace_tx(&entry.tx_hash_hex) {
                    info!("‚úÖ [UDS] TRACE: Transaction {} is in FINAL block {} at position {}", 
                        entry.tx_hash_hex, self.height, idx);
                }
            }
        }
        
        // Collect batch_digests ƒë·ªÉ check khi retry
        let batch_digests: Vec<Option<BatchDigest>> = sorted_entries.iter()
            .map(|e| e.batch_digest)
            .collect();
        
        (
            comm::CommittedBlock {
                epoch: self.epoch,
                height: self.height,
                transactions,
            },
            tx_hash_map,
            batch_digests,
        )
    }
}

impl UdsExecutionState {
    /// Persist execution state to disk
    async fn persist_execution_state(&self) -> Result<(), String> {
        let state_path = match &self.execution_state_path {
            Some(path) => path,
            None => return Ok(()), // No persistence path configured
        };

        let state = {
            let last_consensus_index = *self.last_consensus_index.lock().await;
            let last_sent_height = *self.last_sent_height.lock().await;
            PersistedExecutionState {
                last_consensus_index,
                last_sent_height,
            }
        };

        // Serialize to JSON
        let json = serde_json::to_string_pretty(&state)
            .map_err(|e| format!("Failed to serialize execution state: {}", e))?;

        // Write atomically: write to temp file, then rename
        let temp_path = state_path.with_extension("tmp");
        fs::write(&temp_path, json)
            .map_err(|e| format!("Failed to write execution state to {}: {}", temp_path.display(), e))?;
        
        fs::rename(&temp_path, state_path)
            .map_err(|e| format!("Failed to rename execution state file: {}", e))?;

        debug!("üíæ [UDS] Persisted execution state: last_consensus_index={}, last_sent_height={:?}", 
            state.last_consensus_index, state.last_sent_height);
        
        Ok(())
    }

    /// Load execution state from disk
    async fn load_execution_state(&self) -> Result<PersistedExecutionState, String> {
        let state_path = match &self.execution_state_path {
            Some(path) => path,
            None => return Ok(PersistedExecutionState::default()), // No persistence path configured
        };

        if !state_path.exists() {
            debug!("üíæ [UDS] Execution state file does not exist: {}, using default", state_path.display());
            return Ok(PersistedExecutionState::default());
        }

        let json = fs::read_to_string(state_path)
            .map_err(|e| format!("Failed to read execution state from {}: {}", state_path.display(), e))?;

        let state: PersistedExecutionState = serde_json::from_str(&json)
            .map_err(|e| format!("Failed to deserialize execution state: {}", e))?;

        info!("üíæ [UDS] Loaded execution state: last_consensus_index={}, last_sent_height={:?}", 
            state.last_consensus_index, state.last_sent_height);

        Ok(state)
    }

    pub fn new(
        socket_path: String,
        epoch: u64,
        empty_block_timeout_ms: u64,
    ) -> Self {
        Self::new_with_retry(socket_path, epoch, empty_block_timeout_ms, 3, 100)
    }
    
    pub fn new_with_retry(
        socket_path: String,
        epoch: u64,
        empty_block_timeout_ms: u64,
        max_send_retries: u32,
        retry_delay_base_ms: u64,
    ) -> Self {
        Self::new_with_retry_and_missed_detection(
            socket_path,
            epoch,
            empty_block_timeout_ms,
            max_send_retries,
            retry_delay_base_ms,
            MISSED_BATCH_TIMEOUT_SECS * 1000, // Convert to milliseconds
            MAX_MISSED_BATCH_RETRIES,
        )
    }
    
    pub fn new_with_retry_and_missed_detection(
        socket_path: String,
        epoch: u64,
        empty_block_timeout_ms: u64,
        max_send_retries: u32,
        retry_delay_base_ms: u64,
        missed_batch_timeout_ms: u64,
        max_missed_batch_retries: u32,
    ) -> Self {
        Self::new_with_state_and_stores(
            socket_path,
            epoch,
            empty_block_timeout_ms,
            max_send_retries,
            retry_delay_base_ms,
            missed_batch_timeout_ms,
            max_missed_batch_retries,
            None::<PathBuf>, // execution_state_path
            None::<Arc<ConsensusStore>>, // consensus_store
            None::<CertificateStore>, // certificate_store
        )
    }

    pub fn new_with_state_and_stores(
        socket_path: String,
        epoch: u64,
        empty_block_timeout_ms: u64,
        max_send_retries: u32,
        retry_delay_base_ms: u64,
        missed_batch_timeout_ms: u64,
        max_missed_batch_retries: u32,
        execution_state_path: Option<PathBuf>,
        consensus_store: Option<Arc<ConsensusStore>>,
        certificate_store: Option<CertificateStore>,
    ) -> Self {
        info!("üöÄ [UDS] Creating UdsExecutionState: socket_path='{}', epoch={}, empty_block_timeout_ms={}, max_retries={}, retry_delay_base_ms={}, missed_batch_timeout_ms={}, max_missed_batch_retries={}, execution_state_path={:?}", 
            socket_path, epoch, empty_block_timeout_ms, max_send_retries, retry_delay_base_ms, missed_batch_timeout_ms, max_missed_batch_retries, execution_state_path);
        Self {
            socket_path,
            epoch,
            current_block: Arc::new(Mutex::new(None)),
            last_sent_height: Arc::new(Mutex::new(None)), // None = ch∆∞a g·ª≠i block n√†o
            last_consensus_index: Arc::new(Mutex::new(0)),
            stream: Arc::new(Mutex::new(None)),
            late_certificates: Arc::new(Mutex::new(Vec::new())),
            max_send_retries,
            retry_delay_base_ms,
            processed_batch_digests: Arc::new(Mutex::new(HashMap::new())),
            missed_batches: Arc::new(Mutex::new(HashMap::new())),
            missed_batch_timeout_ms,
            max_missed_batch_retries,
            logged_duplicate_batches: Arc::new(Mutex::new(HashSet::new())),
            execution_state_path,
            consensus_store,
            certificate_store,
            catch_up_threshold: 50, // Default: 50 certificates lag
            catch_up_check_interval: Duration::from_secs(10), // Default: 10 seconds
            persistence_counter: Arc::new(Mutex::new(0)),
        }
    }

    /// Initialize execution state by loading from disk
    /// This should be called after construction to load persisted state
    pub async fn initialize(&self) -> Result<(), String> {
        let loaded_state = self.load_execution_state().await?;
        *self.last_consensus_index.lock().await = loaded_state.last_consensus_index;
        *self.last_sent_height.lock().await = loaded_state.last_sent_height;
        info!("‚úÖ [UDS] Initialized execution state: last_consensus_index={}, last_sent_height={:?}", 
            loaded_state.last_consensus_index, loaded_state.last_sent_height);
        Ok(())
    }

    /// Spawn background task for periodic catch-up checks
    /// This should be called after initialization
    pub fn spawn_catchup_task(self: Arc<Self>) -> tokio::task::JoinHandle<()> {
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(self.catch_up_check_interval);
            loop {
                interval.tick().await;
                
                match self.check_execution_lag().await {
                    Ok(Some(execution_index)) => {
                        // Lag detected, trigger recovery
                        let consensus_store = match &self.consensus_store {
                            Some(store) => store,
                            None => continue, // No consensus store configured
                        };

                        let consensus_index = match consensus_store.read_last_consensus_index() {
                            Ok(idx) => idx,
                            Err(e) => {
                                error!("‚ùå [UDS] Failed to read last consensus index for recovery: {}", e);
                                continue;
                            }
                        };

                        if let Err(e) = self.trigger_recovery(execution_index, consensus_index).await {
                            error!("‚ùå [UDS] Recovery failed: {}", e);
                        }
                    }
                    Ok(None) => {
                        // No lag detected
                        debug!("‚úÖ [UDS] Execution is in sync");
                    }
                    Err(e) => {
                        warn!("‚ö†Ô∏è [UDS] Error checking execution lag: {}", e);
                    }
                }
            }
        })
    }

    /// Check if execution is lagging behind consensus
    /// Returns Some(execution_index) if lag is detected, None otherwise
    async fn check_execution_lag(&self) -> Result<Option<u64>, String> {
        let consensus_store = match &self.consensus_store {
            Some(store) => store,
            None => return Ok(None), // No consensus store configured
        };

        let consensus_next_index = consensus_store
            .read_last_consensus_index()
            .map_err(|e| format!("Failed to read last consensus index: {}", e))?;

        let execution_index = {
            let guard = self.last_consensus_index.lock().await;
            *guard
        };

        if consensus_next_index > execution_index + self.catch_up_threshold {
            warn!("‚ö†Ô∏è [UDS] Execution lag detected: consensus_index={}, execution_index={}, lag={}", 
                consensus_next_index, execution_index, consensus_next_index - execution_index);
            Ok(Some(execution_index))
        } else {
            Ok(None)
        }
    }

    /// Trigger recovery by reading and re-processing missing certificates
    /// CRITICAL: ƒê·∫£m b·∫£o blocks ƒë∆∞·ª£c t·∫°o tu·∫ßn t·ª± v√† g·ª≠i qua UDS trong qu√° tr√¨nh recovery
    /// - X·ª≠ l√Ω certificates tu·∫ßn t·ª± theo consensus_index (deterministic, fork-safe)
    /// - T·∫°o blocks t·ª´ certificates (c√≥ th·ªÉ empty n·∫øu kh√¥ng c√≥ transaction data)
    /// - G·ª≠i blocks qua UDS ƒë·ªÉ ƒë·∫£m b·∫£o execution tƒÉng ti·∫øn
    /// - Flush t·∫•t c·∫£ pending blocks sau recovery
    async fn trigger_recovery(&self, start_index: u64, end_index: u64) -> Result<(), String> {
        let consensus_store = match &self.consensus_store {
            Some(store) => store,
            None => return Err("Consensus store not configured".to_string()),
        };

        let certificate_store = match &self.certificate_store {
            Some(store) => store,
            None => return Err("Certificate store not configured".to_string()),
        };

        info!("üîÑ [UDS] Starting recovery: reading certificates from {} to {} (ensuring sequential block creation and UDS sending)", 
            start_index, end_index - 1);

        // Read missing certificates from consensus store
        // CRITICAL: ƒê·ªçc tu·∫ßn t·ª± theo consensus_index ƒë·ªÉ ƒë·∫£m b·∫£o deterministic v√† fork-safe
        let missing = consensus_store
            .read_sequenced_certificates(&(start_index..=end_index - 1))
            .map_err(|e| format!("Failed to read sequenced certificates: {}", e))?;

        let mut recovered_count = 0;
        let mut last_processed_index = start_index.saturating_sub(1);
        
        // CRITICAL: X·ª≠ l√Ω certificates tu·∫ßn t·ª± theo consensus_index
        // ƒê·∫£m b·∫£o blocks ƒë∆∞·ª£c t·∫°o tu·∫ßn t·ª± v√† g·ª≠i qua UDS
        for (cert_digest_opt, seq) in missing.iter().zip(start_index..end_index) {
            if let Some(cert_digest) = cert_digest_opt {
                // Read certificate from certificate store
                if let Ok(Some(cert)) = certificate_store.read(*cert_digest) {
                    // CRITICAL: ƒê·∫£m b·∫£o x·ª≠ l√Ω tu·∫ßn t·ª± theo consensus_index
                    // handle_consensus_transaction s·∫Ω t·ª± ƒë·ªông x·ª≠ l√Ω gaps th√¥ng qua
                    // fill_missing_blocks v√† flush_current_block_if_needed
                    
                    // Create ConsensusOutput
                    let consensus_output = ConsensusOutput {
                        certificate: cert.clone(),
                        consensus_index: seq,
                    };

                    // Create ExecutionIndices
                    // CRITICAL: T√≠nh to√°n ExecutionIndices ƒë√∫ng ƒë·ªÉ ƒë·∫£m b·∫£o deterministic processing
                    let execution_indices = ExecutionIndices {
                        next_certificate_index: seq,
                        next_batch_index: 0, // Recovery kh√¥ng c√≥ batch index info, d√πng 0
                        next_transaction_index: 0, // Recovery kh√¥ng c√≥ transaction index info, d√πng 0
                    };

                    // CRITICAL: Check xem certificate ƒë√£ ƒë∆∞·ª£c processed ch∆∞a
                    // N·∫øu ƒë√£ processed (consensus_index <= last_consensus_index), skip ƒë·ªÉ tr√°nh duplicate
                    let should_process = {
                        let guard = self.last_consensus_index.lock().await;
                        seq > *guard
                    };
                    
                    if should_process {
                        // CRITICAL: Re-process certificate tu·∫ßn t·ª±
                        // handle_consensus_transaction s·∫Ω:
                        // 1. T·∫°o blocks t·ª´ certificates (c√≥ th·ªÉ empty n·∫øu kh√¥ng c√≥ transaction data)
                        // 2. G·ª≠i blocks qua UDS khi ƒë·ªß BLOCK_SIZE certificates
                        // 3. ƒê·∫£m b·∫£o sequential execution v√† fork-safe
                        // NOTE: Recovery kh√¥ng c√≥ transaction data, ch·ªâ t·∫°o empty blocks ƒë·ªÉ ƒë·∫£m b·∫£o sequential execution
                        self.handle_consensus_transaction(&consensus_output, execution_indices, Vec::new()).await;
                    } else {
                        debug!("‚è≠Ô∏è [UDS] Skipping certificate in recovery: consensus_index={} <= last_consensus_index (already processed)", seq);
                    }
                    
                    last_processed_index = seq;
                    recovered_count += 1;
                } else {
                    warn!("‚ö†Ô∏è [UDS] Certificate not found in store: {:?}, consensus_index={}", cert_digest, seq);
                    // V·∫´n update last_processed_index ƒë·ªÉ kh√¥ng t·∫°o duplicate empty blocks
                    last_processed_index = seq;
                }
            } else {
                // Certificate digest kh√¥ng c√≥ ‚Üí consensus_index n√†y kh√¥ng c√≥ certificate
                // CRITICAL: Kh√¥ng t·∫°o block cho consensus_index kh√¥ng c√≥ certificate
                // Ch·ªâ update last_processed_index ƒë·ªÉ ti·∫øp t·ª•c x·ª≠ l√Ω
                warn!("‚ö†Ô∏è [UDS] Certificate digest not found for consensus_index={}, skipping (no certificate)", seq);
                last_processed_index = seq;
            }
        }

        // CRITICAL: Flush t·∫•t c·∫£ pending blocks sau recovery
        // ƒê·∫£m b·∫£o t·∫•t c·∫£ blocks ƒë∆∞·ª£c g·ª≠i qua UDS, kh√¥ng b·ªè s√≥t
        let last_consensus_index = {
            let guard = self.last_consensus_index.lock().await;
            *guard
        };
        
        // Flush current block n·∫øu c√≥ (ƒë·∫£m b·∫£o block cu·ªëi c√πng ƒë∆∞·ª£c g·ª≠i)
        self.flush_current_block_if_needed(last_consensus_index).await;
        
        // CRITICAL: ƒê·∫£m b·∫£o t·∫•t c·∫£ blocks ƒë√£ ƒë∆∞·ª£c g·ª≠i qua UDS
        // Ki·ªÉm tra v√† fill gaps n·∫øu c·∫ßn (ƒë·∫£m b·∫£o sequential execution v√† kh√¥ng fork)
        let last_sent_height = {
            let guard = self.last_sent_height.lock().await;
            *guard
        };
        
        if let Some(last_sent) = last_sent_height {
            let expected_last_block = last_consensus_index / BLOCK_SIZE;
            if expected_last_block > last_sent {
                // C√≥ blocks ch∆∞a ƒë∆∞·ª£c g·ª≠i ‚Üí fill gaps
                info!("üîÑ [UDS] Filling gaps after recovery: last_sent={}, expected_last_block={}", 
                    last_sent, expected_last_block);
                if let Err(e) = self.send_empty_blocks_for_gaps(last_sent + 1, expected_last_block + 1).await {
                    warn!("‚ö†Ô∏è [UDS] Failed to fill gaps after recovery: {}", e);
                }
            }
        }
        
        info!("‚úÖ [UDS] Recovery completed: recovered {} certificates from {} to {}, last_consensus_index={}", 
            recovered_count, start_index, end_index - 1, last_consensus_index);

        // Persist state after recovery
        if let Err(e) = self.persist_execution_state().await {
            warn!("‚ö†Ô∏è [UDS] Failed to persist execution state after recovery: {}", e);
        }

        Ok(())
    }

    async fn ensure_connection(&self) -> Result<(), String> {
        let mut stream_guard = self.stream.lock().await;
        if stream_guard.is_none() {
            let stream = UnixStream::connect(&self.socket_path)
                .await
                .map_err(|e| format!("Failed to connect to UDS {}: {}", self.socket_path, e))?;
            *stream_guard = Some(stream);
            info!("‚úÖ [UDS] Connected to Unix Domain Socket: {}", self.socket_path);
        }
        Ok(())
    }

    /// Send a single block to UDS (progressive sending, no batching)
    /// G·ª≠i block v·ªõi retry mechanism (exponential backoff)
    /// CRITICAL: Ch·ªâ d·ª±a v√†o last_sent_height ƒë·ªÉ check duplicate
    /// - Check last_sent_height: N·∫øu block.height <= last_sent_height ‚Üí block ƒë√£ ƒë∆∞·ª£c g·ª≠i ‚Üí skip retry
    /// - KH√îNG check processed_batch_digests ·ªü ƒë√¢y v√¨ batch ƒë∆∞·ª£c marked as processed SAU KHI ƒë∆∞·ª£c th√™m v√†o block
    /// - Check processed_batch_digests ch·ªâ d√πng trong handle_consensus_transaction ƒë·ªÉ tr√°nh duplicate execution
    /// - FORK-SAFE: T·∫•t c·∫£ nodes check c√πng last_sent_height ‚Üí c√πng quy·∫øt ƒë·ªãnh skip ‚Üí fork-safe
    async fn send_block_with_retry(&self, block: comm::CommittedBlock, tx_hash_map: HashMap<Vec<u8>, String>, batch_digests: Vec<Option<BatchDigest>>) -> Result<(), String> {
        // CRITICAL: Check xem block ƒë√£ ƒë∆∞·ª£c g·ª≠i th√†nh c√¥ng ch∆∞a (d·ª±a v√†o last_sent_height)
        // CRITICAL: Ch·ªâ d·ª±a v√†o last_sent_height ƒë·ªÉ check duplicate
        // KH√îNG check processed_batch_digests ·ªü ƒë√¢y v√¨:
        // - Batch ƒë∆∞·ª£c marked as processed SAU KHI ƒë∆∞·ª£c th√™m v√†o block
        // - N·∫øu check processed_batch_digests ·ªü ƒë√¢y, s·∫Ω skip block ngay c·∫£ khi batch v·ª´a ƒë∆∞·ª£c th√™m v√†o block hi·ªán t·∫°i
        // - Check processed_batch_digests ch·ªâ n√™n d√πng trong handle_consensus_transaction ƒë·ªÉ tr√°nh duplicate execution
        
        let mut last_error = None;
        
        for attempt in 0..self.max_send_retries {
            // CRITICAL: Check tr√πng l·∫∑p TR∆Ø·ªöC M·ªñI L·∫¶N g·ªçi send_block_internal
            // ƒê·∫£m b·∫£o kh√¥ng retry n·∫øu block ƒë√£ ƒë∆∞·ª£c g·ª≠i
            // 
            // QU√Å TR√åNH CHECK:
            // 1. Check last_sent_height: N·∫øu block.height <= last_sent ‚Üí block ƒë√£ ƒë∆∞·ª£c g·ª≠i ‚Üí skip
            // 2. N·∫øu kh√¥ng c√≥ duplicate ‚Üí g·ªçi send_block_internal
            // 3. N·∫øu send_block_internal fail ‚Üí sleep v√† retry (s·∫Ω check l·∫°i ·ªü iteration ti·∫øp theo)
            
            // Check: last_sent_height (c√°ch ch·∫Øc ch·∫Øn nh·∫•t ƒë·ªÉ bi·∫øt block ƒë√£ ƒë∆∞·ª£c g·ª≠i)
            let last_sent_guard = self.last_sent_height.lock().await;
            if let Some(last_sent) = *last_sent_guard {
                if block.height <= last_sent {
                    drop(last_sent_guard);
                    if attempt > 0 {
                        uds_debug!("‚è≠Ô∏è [UDS] Stopping retry for block {} (attempt {}): Block already sent (last_sent_height={})", 
                            block.height, attempt + 1, last_sent);
                    } else {
                        uds_debug!("‚è≠Ô∏è [UDS] Skipping retry for block {}: Block already sent (last_sent_height={})", 
                            block.height, last_sent);
                    }
                    return Ok(()); // Block ƒë√£ ƒë∆∞·ª£c g·ª≠i th√†nh c√¥ng
                }
            }
            drop(last_sent_guard);
            
            // Kh√¥ng c√≥ duplicate ‚Üí g·ªçi send_block_internal
            match self.send_block_internal(block.clone(), &tx_hash_map).await {
                Ok(_) => {
                    if attempt > 0 {
                        info!("‚úÖ [UDS] Block {} sent successfully after {} retries", block.height, attempt);
                    }
                    return Ok(());
                }
                Err(e) => {
                    last_error = Some(e.clone());
                    if attempt < self.max_send_retries - 1 {
                        // Ch·ªù m·ªôt kho·∫£ng th·ªùi gian tr∆∞·ªõc khi retry (exponential backoff)
                        let delay_ms = self.retry_delay_base_ms * 2_u64.pow(attempt);
                        warn!("‚ö†Ô∏è [UDS] Failed to send block {} (attempt {}/{}): {}. Retrying in {}ms... (will check duplicates before next attempt)", 
                            block.height, attempt + 1, self.max_send_retries, e, delay_ms);
                        sleep(TokioDuration::from_millis(delay_ms)).await;
                        // L∆∞u √Ω: Check tr√πng l·∫∑p s·∫Ω ƒë∆∞·ª£c th·ª±c hi·ªán l·∫°i ·ªü ƒë·∫ßu v√≤ng l·∫∑p ti·∫øp theo
                    }
                }
            }
        }
        
        Err(format!("Failed to send block {} after {} retries: {:?}", 
            block.height, self.max_send_retries, last_error))
    }
    
    /// Internal method ƒë·ªÉ g·ª≠i block (kh√¥ng retry)
    /// tx_hash_map: Map t·ª´ transaction digest bytes ‚Üí tx_hash_hex (ƒë·ªÉ log hash ch√≠nh x√°c)
    /// 
    /// CRITICAL: ƒê·∫£m b·∫£o bytes g·ªëc ƒë∆∞·ª£c gi·ªØ nguy√™n:
    /// - tx.digest ch·ª©a transaction bytes G·ªêC (kh√¥ng serialize l·∫°i)
    /// - Protobuf encode ch·ªâ serialize message structure, kh√¥ng thay ƒë·ªïi bytes trong digest field
    /// - Go side s·∫Ω nh·∫≠n ƒë∆∞·ª£c ƒë√∫ng bytes g·ªëc v√† parse ƒë∆∞·ª£c ‚Üí hash s·∫Ω kh·ªõp
    async fn send_block_internal(&self, block: comm::CommittedBlock, tx_hash_map: &HashMap<Vec<u8>, String>) -> Result<(), String> {
        // CRITICAL: Validate bytes g·ªëc TR∆Ø·ªöC KHI encode protobuf
        // ƒê·∫£m b·∫£o transaction bytes trong block l√† bytes g·ªëc, kh√¥ng b·ªã thay ƒë·ªïi
        if !block.transactions.is_empty() {
            for (idx, tx) in block.transactions.iter().enumerate() {
                let digest_key = tx.digest.as_ref().to_vec();
                if let Some(expected_hash) = tx_hash_map.get(&digest_key) {
                    // Validate: Parse wrapper bytes v√† t√≠nh hash cho transaction ƒë·∫ßu ti√™n ƒë·ªÉ ƒë·∫£m b·∫£o kh·ªõp
                    // CRITICAL: digest bytes l√† Transactions wrapper ‚Üí parse nh∆∞ Transactions v√† l·∫•y transaction ƒë·∫ßu ti√™n
                    match transaction::Transactions::decode(tx.digest.as_ref()) {
                        Ok(parsed_wrapper) => {
                            if parsed_wrapper.transactions.is_empty() {
                                error!("‚ùå [UDS] CRITICAL: Wrapper bytes contains 0 transactions! Block {} Tx[{}]: DigestLen={}", 
                                    block.height, idx, tx.digest.len());
                                return Err(format!("CRITICAL: Wrapper bytes is empty! Block {} Tx[{}]", block.height, idx));
                            }
                            // Validate hash c·ªßa transaction ƒë·∫ßu ti√™n trong wrapper (v√¨ tx_hash_map ch·ªâ map wrapper ‚Üí hash c·ªßa transaction ƒë·∫ßu ti√™n)
                            let first_tx = &parsed_wrapper.transactions[0];
                            let validation_hash = calculate_transaction_hash_from_proto(first_tx);
                            let validation_hash_hex = hex::encode(&validation_hash);
                            if validation_hash_hex != *expected_hash {
                                error!("‚ùå [UDS] CRITICAL: Hash mismatch before protobuf encode! Block {} Tx[{}]: Expected={}, Calculated={}, DigestLen={}, WrapperTxCount={}. Bytes may have been corrupted!", 
                                    block.height, idx, expected_hash, validation_hash_hex, tx.digest.len(), parsed_wrapper.transactions.len());
                                return Err(format!("CRITICAL: Hash validation failed before encode - transaction bytes corrupted! Block {} Tx[{}]", block.height, idx));
                            }
                            // Hash kh·ªõp ‚Üí wrapper bytes ƒë√∫ng
                            uds_debug!("‚úÖ [UDS] Pre-encode validation: Block {} Tx[{}] wrapper bytes verified: TxHash={}, BytesLen={}, WrapperTxCount={}", 
                                block.height, idx, expected_hash, tx.digest.len(), parsed_wrapper.transactions.len());
                        }
                        Err(e) => {
                            error!("‚ùå [UDS] CRITICAL: Cannot parse digest bytes as Transactions wrapper before encode! Block {} Tx[{}]: DigestLen={}, Error: {:?}. Bytes may have been corrupted!", 
                                block.height, idx, tx.digest.len(), e);
                            return Err(format!("CRITICAL: Cannot validate wrapper bytes before encode - parsing failed! Block {} Tx[{}]", block.height, idx));
                        }
                    }
                }
            }
        }
        
        let epoch_data = comm::CommittedEpochData {
            blocks: vec![block.clone()],
        };

        // Encode protobuf
        // CRITICAL: Protobuf encode ch·ªâ serialize message structure (field tags, lengths)
        // - Transaction bytes trong tx.digest ƒë∆∞·ª£c gi·ªØ NGUY√äN V·∫∏N (protobuf ch·ªâ wrap bytes, kh√¥ng thay ƒë·ªïi n·ªôi dung)
        // - Go side s·∫Ω nh·∫≠n ƒë∆∞·ª£c ƒë√∫ng bytes g·ªëc t·ª´ tx.digest field
        let mut proto_buf = Vec::new();
        Message::encode(&epoch_data, &mut proto_buf)
            .map_err(|e| format!("Failed to encode protobuf: {}", e))?;

        // Log tr∆∞·ªõc khi g·ª≠i (ch·ªâ log khi c√≥ transaction)
        if !block.transactions.is_empty() {
            info!("üì§ [UDS] Preparing to send block: Height={}, Epoch={}, TxCount={}, ProtoSize={} bytes", 
                block.height, block.epoch, block.transactions.len(), proto_buf.len());
            
            // Log t·∫•t c·∫£ transaction hashes tr∆∞·ªõc khi g·ª≠i (d√πng hash ƒë√£ l∆∞u s·∫µn t·ª´ tx_hash_map)
            // CRITICAL: ƒê·∫£m b·∫£o hash nh·∫•t qu√°n t·ª´ khi th√™m v√†o block ƒë·∫øn khi g·ª≠i sang UDS
            info!("üìã [UDS] Block {} transaction hashes before sending to UDS:", block.height);
            for (idx, tx) in block.transactions.iter().enumerate() {
                let digest_key = tx.digest.as_ref().to_vec();
                if let Some(tx_hash_hex) = tx_hash_map.get(&digest_key) {
                    info!("  üîπ [UDS] Block {} Tx[{}]: TxHash={}, WorkerId={}, DigestLen={} bytes", 
                        block.height, idx, tx_hash_hex, tx.worker_id, tx.digest.len());
                    
                    // CRITICAL: Validate hash consistency - hash trong log ph·∫£i kh·ªõp v·ªõi hash ƒë√£ l∆∞u
                    debug!("  ‚úÖ [UDS] Block {} Tx[{}]: Hash validated - TxHash={} matches stored hash", 
                        block.height, idx, tx_hash_hex);
                } else {
                    // Fallback: t√≠nh hash n·∫øu kh√¥ng t√¨m th·∫•y trong map (KH√îNG N√äN X·∫¢Y RA)
                    error!("  ‚ùå [UDS] Block {} Tx[{}]: CRITICAL - Hash not found in map! WorkerId={}, DigestLen={} bytes, MapSize={}", 
                        block.height, idx, tx.worker_id, tx.digest.len(), tx_hash_map.len());
                    
                    // CRITICAL: digest bytes l√† Transactions wrapper ‚Üí parse nh∆∞ Transactions v√† l·∫•y transaction ƒë·∫ßu ti√™n
                    match transaction::Transactions::decode(tx.digest.as_ref()) {
                        Ok(parsed_wrapper) => {
                            if !parsed_wrapper.transactions.is_empty() {
                                let first_tx = &parsed_wrapper.transactions[0];
                                let fallback_hash = calculate_transaction_hash_from_proto(first_tx);
                                let fallback_hash_hex = hex::encode(&fallback_hash);
                                error!("  ‚ùå [UDS] Block {} Tx[{}]: Calculated hash from wrapper (first tx): TxHash={}, WrapperTxCount={} (NOT in map - hash may differ!)", 
                                    block.height, idx, fallback_hash_hex, parsed_wrapper.transactions.len());
                            } else {
                                error!("  ‚ùå [UDS] Block {} Tx[{}]: Wrapper bytes is empty!", block.height, idx);
                            }
                        }
                        Err(e) => {
                            error!("  ‚ùå [UDS] Block {} Tx[{}]: Failed to parse digest as Transactions wrapper: Error={:?}", 
                                block.height, idx, e);
                        }
                    }
                }
            }
        } else {
            uds_debug!("üì§ [UDS] Preparing to send EMPTY block: Height={}, Epoch={}, ProtoSize={} bytes", 
                block.height, block.epoch, proto_buf.len());
        }

        // Send via UDS
        // CRITICAL: ƒê·∫£m b·∫£o d·ªØ li·ªáu nh·∫•t qu√°n - transaction bytes trong proto_buf ph·∫£i kh·ªõp v·ªõi tx.digest
        self.ensure_connection().await?;
        let mut stream_guard = self.stream.lock().await;
        if let Some(stream) = stream_guard.as_mut() {
            // VALIDATION: Verify transaction bytes trong block kh·ªõp v·ªõi tx_hash_map
            // ƒê·∫£m b·∫£o bytes ƒë∆∞·ª£c g·ª≠i l√† bytes ƒë√∫ng v√† hash s·∫Ω kh·ªõp v·ªõi Go side
            if !block.transactions.is_empty() {
                for (idx, tx) in block.transactions.iter().enumerate() {
                    let digest_key = tx.digest.as_ref().to_vec();
                    if let Some(expected_hash) = tx_hash_map.get(&digest_key) {
                        // CRITICAL: Log hex c·ªßa digest bytes tr∆∞·ªõc khi g·ª≠i (ch·ªâ cho transaction ƒë∆∞·ª£c trace)
                        if should_trace_tx(expected_hash) {
                            let digest_hex = hex::encode(tx.digest.as_ref());
                            info!("üîç [UDS] TRACE: Digest bytes hex for {} BEFORE sending to UDS: {} (first 100 chars: {})", 
                                expected_hash,
                                if digest_hex.len() > 200 { format!("{}...", &digest_hex[..200]) } else { digest_hex.clone() },
                                if digest_hex.len() > 100 { &digest_hex[..100] } else { &digest_hex });
                        }
                        
                        // Double-check: T√≠nh hash t·ª´ wrapper bytes ƒë·ªÉ ƒë·∫£m b·∫£o kh·ªõp
                        // CRITICAL: digest bytes l√† Transactions wrapper ‚Üí parse nh∆∞ Transactions v√† l·∫•y transaction ƒë·∫ßu ti√™n
                        match transaction::Transactions::decode(tx.digest.as_ref()) {
                            Ok(parsed_wrapper) => {
                                if parsed_wrapper.transactions.is_empty() {
                                    error!("‚ùå [UDS] CRITICAL: Wrapper bytes contains 0 transactions before sending! Block {} Tx[{}]: DigestLen={}", 
                                        block.height, idx, tx.digest.len());
                                    panic!("CRITICAL: Wrapper bytes is empty before sending!");
                                }
                                // Validate hash c·ªßa transaction ƒë·∫ßu ti√™n trong wrapper
                                let first_tx = &parsed_wrapper.transactions[0];
                                let validation_hash = calculate_transaction_hash_from_proto(first_tx);
                                let validation_hash_hex = hex::encode(&validation_hash);
                                if validation_hash_hex != *expected_hash {
                                    error!("‚ùå [UDS] CRITICAL: Hash mismatch before sending to UDS! Block {} Tx[{}]: Expected={}, Calculated={}, DigestLen={}, WrapperTxCount={}", 
                                        block.height, idx, expected_hash, validation_hash_hex, tx.digest.len(), parsed_wrapper.transactions.len());
                                    
                                    // CRITICAL: Log hex c·ªßa digest bytes khi hash mismatch
                                    if should_trace_tx(expected_hash) {
                                        let digest_hex = hex::encode(tx.digest.as_ref());
                                        error!("‚ùå [UDS] TRACE: Digest bytes hex when hash mismatch for {}: {} (full: {})", 
                                            expected_hash,
                                            if digest_hex.len() > 200 { format!("{}...", &digest_hex[..200]) } else { digest_hex.clone() },
                                            digest_hex);
                                    }
                                    
                                    panic!("CRITICAL: Hash validation failed before sending - wrapper bytes corrupted!");
                                } else {
                                    uds_debug!("‚úÖ [UDS] Pre-send validation: Block {} Tx[{}] wrapper bytes verified: TxHash={}, BytesLen={}, WrapperTxCount={}", 
                                        block.height, idx, expected_hash, tx.digest.len(), parsed_wrapper.transactions.len());
                                }
                            }
                            Err(e) => {
                                error!("‚ùå [UDS] CRITICAL: Cannot parse digest bytes as Transactions wrapper before sending! Block {} Tx[{}]: DigestLen={}, Error: {:?}", 
                                    block.height, idx, tx.digest.len(), e);
                                
                                // CRITICAL: Log hex c·ªßa digest bytes khi parse failed
                                if should_trace_tx(expected_hash) {
                                    let digest_hex = hex::encode(tx.digest.as_ref());
                                    error!("‚ùå [UDS] TRACE: Digest bytes hex when parse failed for {}: {} (full: {})", 
                                        expected_hash,
                                        if digest_hex.len() > 200 { format!("{}...", &digest_hex[..200]) } else { digest_hex.clone() },
                                        digest_hex);
                                }
                                
                                panic!("CRITICAL: Cannot validate wrapper bytes before sending - parsing failed!");
                            }
                        }
                    }
                }
            }
            
            // Write length prefix (2 bytes, little-endian)
            let len_buf = (proto_buf.len() as u16).to_le_bytes();
            stream.write_all(&len_buf)
                .await
                .map_err(|e| format!("Failed to write length to UDS: {}", e))?;
            
            // Write protobuf data
            // CRITICAL: proto_buf ch·ª©a CommittedEpochData v·ªõi CommittedBlock, 
            // m·ªói block ch·ª©a transactions v·ªõi tx.digest l√† transaction bytes G·ªêC
            // 
            // QUAN TR·ªåNG V·ªÄ BYTES G·ªêC:
            // - tx.digest ch·ª©a transaction bytes g·ªëc (ƒë√£ extract t·ª´ wrapper ho·∫∑c nh·∫≠n tr·ª±c ti·∫øp)
            // - Khi protobuf serialize tx.digest (ki·ªÉu bytes field), n√≥ ch·ªâ th√™m field tag + length prefix
            // - Raw transaction bytes ƒë∆∞·ª£c gi·ªØ NGUY√äN V·∫∏N trong protobuf message
            // - Go side s·∫Ω nh·∫≠n ƒë∆∞·ª£c ƒë√∫ng transaction bytes g·ªëc t·ª´ tx.digest field
            // - KH√îNG c√≥ serialization l·∫°i transaction - ch·ªâ serialize protobuf message structure
            //
            // Bytes n√†y s·∫Ω ƒë∆∞·ª£c Go side parse v√† t√≠nh hash - ph·∫£i kh·ªõp v·ªõi hash ƒë√£ l∆∞u
            stream.write_all(&proto_buf)
                .await
                .map_err(|e| format!("Failed to write block to UDS: {}", e))?;
            
            stream.flush()
                .await
                .map_err(|e| format!("Failed to flush UDS stream: {}", e))?;

            if !block.transactions.is_empty() {
                info!("‚úÖ [UDS] Successfully sent block {} to Unix Domain Socket: Height={}, Epoch={}, TxCount={}, TotalBytes={} (len_buf=2 + proto={})", 
                    block.height, block.height, block.epoch, block.transactions.len(), proto_buf.len() + 2, proto_buf.len());
                
                // Log t·∫•t c·∫£ transaction hashes sau khi g·ª≠i th√†nh c√¥ng (d√πng hash ƒë√£ l∆∞u s·∫µn t·ª´ tx_hash_map)
                // CRITICAL: Hash n√†y ph·∫£i kh·ªõp v·ªõi hash khi th√™m v√†o block v√† khi g·ª≠i sang UDS
                info!("‚úÖ [UDS] Block {} transaction hashes sent to UDS:", block.height);
                for (idx, tx) in block.transactions.iter().enumerate() {
                    let digest_key = tx.digest.as_ref().to_vec();
                    if let Some(tx_hash_hex) = tx_hash_map.get(&digest_key) {
                        info!("  ‚úÖ [UDS] Block {} Tx[{}] sent: TxHash={}, WorkerId={}, DigestLen={} bytes", 
                            block.height, idx, tx_hash_hex, tx.worker_id, tx.digest.len());
                        
                        // CRITICAL: Log ƒë·ªÉ trace giao d·ªãch ƒë∆∞·ª£c g·ª≠i th√†nh c√¥ng
                        if should_trace_tx(tx_hash_hex) {
                            info!("‚úÖ [UDS] TRACE: Transaction {} was SENT to UDS in block {} at position {}", 
                                tx_hash_hex, block.height, idx);
                        }
                        
                        // CRITICAL: Confirm hash consistency - hash n√†y s·∫Ω ƒë∆∞·ª£c Go side d√πng ƒë·ªÉ verify transaction
                        debug!("  ‚úÖ [UDS] Block {} Tx[{}]: Hash confirmed - TxHash={} is consistent throughout block processing", 
                            block.height, idx, tx_hash_hex);
                    } else {
                        // Fallback: t√≠nh hash n·∫øu kh√¥ng t√¨m th·∫•y trong map (KH√îNG N√äN X·∫¢Y RA)
                        error!("  ‚ùå [UDS] Block {} Tx[{}]: CRITICAL - Hash not found in map after sending! WorkerId={}, DigestLen={} bytes", 
                            block.height, idx, tx.worker_id, tx.digest.len());
                        
                        // CRITICAL: digest bytes l√† transaction bytes (KH√îNG ph·∫£i wrapper) ‚Üí parse nh∆∞ Transaction (single)
                        match transaction::Transaction::decode(tx.digest.as_ref()) {
                            Ok(parsed_tx) => {
                                let fallback_hash = calculate_transaction_hash_from_proto(&parsed_tx);
                                let fallback_hash_hex = hex::encode(&fallback_hash);
                                error!("  ‚ùå [UDS] Block {} Tx[{}]: Calculated hash: TxHash={} (NOT in map - transaction may be rejected by Go side!)", 
                                    block.height, idx, fallback_hash_hex);
                            }
                            Err(e) => {
                                error!("  ‚ùå [UDS] Block {} Tx[{}]: Failed to parse digest as Transaction: Error={:?}", 
                                    block.height, idx, e);
                            }
                        }
                    }
                }
            }
        }
        Ok(())
    }

    /// Send empty blocks for missing heights (gaps)
    async fn send_empty_blocks_for_gaps(&self, from_height: u64, to_height: u64) -> Result<(), String> {
        if from_height >= to_height {
            return Ok(());
        }

        let gap_count = to_height - from_height;
        info!("üîó [UDS] Filling gaps: Sending {} empty blocks from height {} to {}", 
            gap_count, from_height, to_height - 1);

        for height in from_height..to_height {
            let empty_block = comm::CommittedBlock {
                epoch: self.epoch,
                height,
                transactions: Vec::new(),
            };
            let empty_tx_hash_map = HashMap::new();
            // Empty block kh√¥ng c√≥ batch_digests
            let empty_batch_digests = Vec::new();
            if let Err(e) = self.send_block_with_retry(empty_block, empty_tx_hash_map, empty_batch_digests).await {
                error!("‚ùå [UDS] Failed to send empty block for gap at height {} after retries: {}", height, e);
                return Err(format!("Failed to send empty block height {}: {}", height, e));
            }
            *self.last_sent_height.lock().await = Some(height);
        }

        uds_debug!("‚úÖ [UDS] Successfully filled gaps: {} empty blocks sent", gap_count);
        Ok(())
    }
}

#[async_trait]
impl ExecutionState for UdsExecutionState {
    /// X·ª≠ l√Ω transaction t·ª´ consensus v√† t·∫°o block.
    /// 
    /// CRITICAL: CH·ªà GOM CERTIFICATES ƒê√É ƒê∆Ø·ª¢C COMMIT
    /// - H√†m n√†y ch·ªâ nh·∫≠n ConsensusOutput - ƒë√¢y l√† certificates ƒê√É ƒê∆Ø·ª¢C CONSENSUS COMMIT
    /// - Certificates ch∆∞a commit KH√îNG BAO GI·ªú ƒë∆∞·ª£c gom v√†o block
    /// - Sau khi commit, consensus s·∫Ω g·ª≠i ConsensusOutput ‚Üí m·ªõi ƒë∆∞·ª£c gom v√†o block
    /// 
    /// LOGIC M·ªöI THEO THU·∫¨T TO√ÅN ƒê·ªíNG THU·∫¨N:
    /// - Round L·∫∫ ch·ªâ d√πng ƒë·ªÉ VOTE/SUPPORT, KH√îNG commit tr·ª±c ti·∫øp
    /// - CH·ªà round CH·∫¥N (leader round) m·ªõi ƒë∆∞·ª£c COMMIT
    /// - Khi round ch·∫µn ƒë∆∞·ª£c commit, n√≥ commit t·∫•t c·∫£ certificates trong sub-DAG (c·∫£ ch·∫µn v√† l·∫ª)
    /// - Block height = round_ch·∫µn / 2
    /// 
    /// 2 TR∆Ø·ªúNG H·ª¢P:
    /// 1. Round ch·∫µn ƒë∆∞·ª£c commit ‚Üí g·ªôp T·∫§T C·∫¢ certificates ƒê√É COMMIT (c·∫£ ch·∫µn v√† l·∫ª t·ª´ sub-DAG) v√†o 1 block
    /// 2. Round ch·∫µn kh√¥ng commit ‚Üí t·∫°o block r·ªóng cho round ch·∫µn ƒë√≥ (KH√îNG c√≥ certificates n√†o v√¨ ch∆∞a commit)
    /// 
    /// Gap: N·∫øu c√≥ gap gi·ªØa c√°c round ch·∫µn (v√≠ d·ª•: round 2 ‚Üí round 6, skip round 4), 
    /// th√¨ round 4 kh√¥ng commit ‚Üí t·∫°o block r·ªóng cho round 4 (KH√îNG gom certificates t·ª´ round 4 v√¨ ch∆∞a commit)
    /// 
    /// QUAN TR·ªåNG: Batch ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t l·∫°i (reproposed)
    /// - Batch c·ªßa round b·ªã skip c√≥ th·ªÉ ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t l·∫°i v√†o round kh√°c
    /// - Khi round m·ªõi ƒë∆∞·ª£c commit, batch ƒë√≥ s·∫Ω ƒë∆∞·ª£c commit v·ªõi certificate m·ªõi (round m·ªõi)
    /// - Certificate m·ªõi n√†y s·∫Ω ƒë∆∞·ª£c GOM V√ÄO BLOCK C·ª¶A ROUND M·ªöI (kh√¥ng ph·∫£i round c≈©)
    /// - V√≠ d·ª•:
    /// X·ª≠ l√Ω consensus transaction d·ª±a ho√†n to√†n v√†o consensus_index
    /// 
    /// Logic m·ªõi:
    /// - Block height = consensus_index / BLOCK_SIZE
    /// - G·ªôp BLOCK_SIZE consensus_index th√†nh 1 block
    /// - G·ª≠i block khi consensus_index >= (block_height + 1) * BLOCK_SIZE
    /// - ƒê·∫£m b·∫£o: T·∫•t c·∫£ certificates v·ªõi consensus_index < (block_height + 1) * BLOCK_SIZE ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
    /// 
    /// ∆Øu ƒëi·ªÉm:
    /// - Kh√¥ng b·ªè s√≥t: consensus_index tu·∫ßn t·ª± tuy·ªát ƒë·ªëi
    /// - Kh√¥ng lo v·ªÅ async processing: Kh√¥ng ph·ª• thu·ªôc v√†o leader_round
    /// - Fork-safe: Deterministic
    /// - Latency t·ªët: G·ª≠i ngay khi ƒë·ªß certificates
    async fn handle_consensus_transaction(
        &self,
        consensus_output: &ConsensusOutput,
        execution_indices: ExecutionIndices,
        transaction: Vec<u8>,
    ) {
        let round = consensus_output.certificate.round();
        let consensus_index = consensus_output.consensus_index;
        let has_transaction = !transaction.is_empty();
        
        // CRITICAL: consensus_output.certificate l√† certificate ƒê√É ƒê∆Ø·ª¢C CONSENSUS COMMIT
        // Consensus ch·ªâ g·ª≠i ConsensusOutput cho certificates ƒë√£ commit th√†nh c√¥ng
        // Certificates ch∆∞a commit KH√îNG BAO GI·ªú c√≥ trong ConsensusOutput ‚Üí kh√¥ng th·ªÉ gom v√†o block
        
        // CRITICAL: Ch·ªâ round CH·∫¥N m·ªõi ƒë∆∞·ª£c commit (leader round)
        // Round l·∫ª ch·ªâ vote/support, kh√¥ng commit tr·ª±c ti·∫øp
        // Khi round ch·∫µn ƒë∆∞·ª£c commit, n√≥ commit t·∫•t c·∫£ certificates trong sub-DAG (c·∫£ ch·∫µn v√† l·∫ª)
        // T·∫§T C·∫¢ certificates trong sub-DAG ƒë·ªÅu c√≥ ConsensusOutput ‚Üí ƒë·ªÅu ƒë∆∞·ª£c gom v√†o block
        
        // CRITICAL: ƒê·∫£m b·∫£o ch·ªâ x·ª≠ l√Ω transactions t·ª´ certificates ƒê√É ƒê∆Ø·ª¢C COMMIT
        // 
        // V·∫§N ƒê·ªÄ: Nhi·ªÅu transactions trong c√πng batch c√≥ C√ôNG consensus_index
        // - T·ª´ notifier.rs: M·ªói transaction trong batch ƒë∆∞·ª£c g·ªçi handle_consensus_transaction ri√™ng bi·ªát
        // - T·∫•t c·∫£ transactions trong c√πng batch c√≥ c√πng ConsensusOutput (c√πng consensus_index)
        // - N·∫øu ch·ªâ check b·∫±ng consensus_index ‚Üí transaction th·ª© 2, 3, ... trong batch s·∫Ω b·ªã skip
        //
        // GI·∫¢I PH√ÅP: D√πng ExecutionIndices ƒë·ªÉ track thay v√¨ ch·ªâ consensus_index
        // - ExecutionIndices bao g·ªìm: (next_certificate_index, next_batch_index, next_transaction_index)
        // - M·ªói transaction c√≥ ExecutionIndices unique ‚Üí kh√¥ng b·ªã skip
        //
        // FORK-SAFETY: ExecutionIndices l√† deterministic t·ª´ consensus
        // - T·∫•t c·∫£ nodes nh·∫≠n c√πng ExecutionIndices sequence ‚Üí c√πng quy·∫øt ƒë·ªãnh x·ª≠ l√Ω
        let mut last_consensus_guard = self.last_consensus_index.lock().await;
        
        // CRITICAL: Ch·ªâ check duplicate b·∫±ng consensus_index n·∫øu consensus_index GI·∫¢M (certificate c≈© h∆°n)
        // N·∫øu consensus_index B·∫∞NG ‚Üí c√≥ th·ªÉ l√† transaction kh√°c trong c√πng batch ‚Üí KH√îNG skip
        // Ch·ªâ skip n·∫øu consensus_index < last (certificate c≈© h∆°n, ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω t·ª´ batch tr∆∞·ªõc)
        if consensus_index < *last_consensus_guard {
            // Certificate n√†y c≈© h∆°n (consensus_index < last) ‚Üí ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω r·ªìi
            // FORK-SAFETY: T·∫•t c·∫£ nodes c√≥ c√πng last_consensus_index ‚Üí c√πng skip
            if has_transaction {
                // Parse transaction ƒë·ªÉ l·∫•y hash cho logging chi ti·∫øt
                let parsed_txs = if !transaction.is_empty() {
                    parse_transactions_from_bytes(&transaction)
                } else {
                    Vec::new()
                };
                for (tx_hash_hex, _, _, _) in parsed_txs {
                    warn!(
                        "‚è≠Ô∏è [UDS] Skipping old certificate with transaction: Round={}, ConsensusIndex={} < LastConsensusIndex={}, TxHash={}",
                        round, consensus_index, *last_consensus_guard, tx_hash_hex
                    );
                    // CRITICAL: Log ƒë·ªÉ trace giao d·ªãch b·ªã skip do old certificate
                    if should_trace_tx(&tx_hash_hex) {
                        error!("‚ùå [UDS] TRACE: Transaction {} was SKIPPED due to old certificate (Round={}, ConsensusIndex={} < LastConsensusIndex={})", 
                            tx_hash_hex, round, consensus_index, *last_consensus_guard);
                    }
                }
            }
            debug!(
                "‚è≠Ô∏è [UDS] Skipping old certificate: consensus_index {} < last_consensus_index {} (Round={})",
                consensus_index, *last_consensus_guard, round
            );
            return; // Certificate c≈© ‚Üí skip (fork-safe)
        }
        
        // Certificate m·ªõi ho·∫∑c c√πng consensus_index (transaction kh√°c trong c√πng batch)
        // Update last_consensus_index ch·ªâ khi consensus_index TƒÇNG (certificate m·ªõi h∆°n)
        // FORK-SAFETY: Update tr∆∞·ªõc khi x·ª≠ l√Ω ƒë·∫£m b·∫£o t·∫•t c·∫£ nodes c√πng update
        if consensus_index > *last_consensus_guard {
            *last_consensus_guard = consensus_index;
        }
        drop(last_consensus_guard);
        
        // Periodic persistence: persist every 10 certificates
        // Note: Persistence is done synchronously but quickly (just file I/O)
        let mut counter_guard = self.persistence_counter.lock().await;
        *counter_guard += 1;
        let should_persist_now = *counter_guard >= 10; // Persist every 10 certificates
        if should_persist_now {
            *counter_guard = 0;
            drop(counter_guard);
            // Persist execution state (non-blocking file I/O)
            if let Err(e) = self.persist_execution_state().await {
                warn!("‚ö†Ô∏è [UDS] Failed to persist execution state: {}", e);
            }
        }
        
        // CRITICAL: Extract batch digest t·ª´ certificate payload ƒë·ªÉ check duplicate TR∆Ø·ªöC KHI parse/log
        // T·ªëi ∆∞u: Check duplicate s·ªõm ƒë·ªÉ tr√°nh parse/log kh√¥ng c·∫ßn thi·∫øt khi batch ƒë√£ processed
        let batch_index_in_payload = execution_indices.next_batch_index.saturating_sub(1) as usize;
        let batch_digest_opt = consensus_output
            .certificate
            .header
            .payload
            .iter()
            .nth(batch_index_in_payload)
            .map(|(digest, _)| *digest);
        
        // T·ªêI ∆ØU + FORK-SAFE: Check duplicate batch - ch·ªâ check processed_batch_digests
        // Logic: N·∫øu batch ƒë√£ processed v·ªõi consensus_index kh√°c ‚Üí skip (duplicate)
        // FORK-SAFETY: T·∫•t c·∫£ nodes check c√πng processed_batch_digests ‚Üí c√πng quy·∫øt ƒë·ªãnh skip ‚Üí fork-safe
        // CRITICAL: GC ph·∫£i deterministic (d·ª±a tr√™n consensus_index) ƒë·ªÉ ƒë·∫£m b·∫£o t·∫•t c·∫£ nodes c√≥ c√πng state
        // PERFORMANCE: Fast lookup - kh√¥ng blocking consensus
        if let Some(batch_digest) = batch_digest_opt {
            // Fast check: processed_batch_digests (read-only, minimal lock time)
            let processed_consensus_index_opt = {
                let processed_batch_guard = self.processed_batch_digests.lock().await;
                processed_batch_guard.get(&batch_digest).copied()
            };
            
            if let Some(processed_consensus_index) = processed_consensus_index_opt {
                if processed_consensus_index != consensus_index {
                    // Batch ƒë√£ processed v·ªõi consensus_index kh√°c ‚Üí skip duplicate
                    // FORK-SAFE: T·∫•t c·∫£ nodes c√≥ c√πng processed_batch_digests ‚Üí c√πng quy·∫øt ƒë·ªãnh skip
                    
                    // Fast log: logged_duplicate_batches (minimal lock time)
                    let should_log = {
                        let mut logged_guard = self.logged_duplicate_batches.lock().await;
                        let inserted = logged_guard.insert(batch_digest.clone());
                        // Lazy GC: ch·ªâ khi cache qu√° l·ªõn
                        const MAX_LOGGED_DUPLICATES: usize = 1000;
                        if logged_guard.len() > MAX_LOGGED_DUPLICATES {
                            logged_guard.clear(); // Fast clear
                        }
                        inserted
                    };
                    
                    if should_log {
                        debug!("‚è≠Ô∏è [UDS] Skipping duplicate batch: BatchDigest={:?}, ConsensusIndex={} (already processed with {})", 
                            batch_digest, consensus_index, processed_consensus_index);
                    }
                    return; // Skip duplicate (fork-safe, fast return)
                }
            }
        }
        
        // CRITICAL: 1 batch ch·ª©a m·ªôt m·∫£ng giao d·ªãch
        // Parse transaction bytes - c√≥ th·ªÉ l√† Transactions protobuf (nhi·ªÅu transactions) ho·∫∑c Transaction (single)
        // T√≠nh hash cho T·∫§T C·∫¢ transactions t·ª´ TransactionHashData (protobuf encoded) ƒë·ªÉ ƒë·∫£m b·∫£o kh·ªõp v·ªõi Go
        let parsed_transactions = if !transaction.is_empty() {
            parse_transactions_from_bytes(&transaction)
        } else {
            Vec::new()
        };
        
        let tx_count = parsed_transactions.len();
        
        // CRITICAL: Log info cho certificate c√≥ transaction v·ªõi hash ƒë·ªÉ trace (GI·ªêNG WORKER)
        if has_transaction {
            let tx_hex_full = hex::encode(&transaction);
            let block_height = consensus_index / BLOCK_SIZE;
            
            if tx_count > 1 {
                // Nhi·ªÅu transactions trong Transactions protobuf - log t·ª´ng transaction
                info!(
                    "[PRIMARY] Processing certificate: Round={}, ConsensusIndex={}, BlockHeight={}, TxCount={}",
                    round, consensus_index, block_height, tx_count
                );
                for (idx, (tx_hash_hex, _tx_hash, _tx_proto, _raw_bytes)) in parsed_transactions.iter().enumerate() {
                    info!(
                        "[PRIMARY] Certificate transaction [{}/{}]: Round={}, ConsensusIndex={}, BlockHeight={}, TxHash={}, TxHex={}, Size={} bytes",
                        idx + 1,
                        tx_count,
                        round,
                        consensus_index,
                        block_height,
                        tx_hash_hex,
                        tx_hex_full,
                        transaction.len()
                    );
                }
            } else if tx_count == 1 {
                // Single transaction
                let tx_hash_hex = &parsed_transactions[0].0;
                info!(
                    "[PRIMARY] Processing certificate: Round={}, ConsensusIndex={}, BlockHeight={}, TxHash={}, TxHex={}, Size={} bytes",
                    round, consensus_index, block_height, tx_hash_hex, tx_hex_full, transaction.len()
                );
            } else {
                // Kh√¥ng parse ƒë∆∞·ª£c transaction (fallback)
                info!(
                    "[PRIMARY] Processing certificate: Round={}, ConsensusIndex={}, BlockHeight={}, HasTransaction=true but failed to parse, TxHex={}, Size={} bytes",
                    round, consensus_index, block_height, tx_hex_full, transaction.len()
                );
            }
        }
        
        // CRITICAL: Block height = consensus_index / BLOCK_SIZE
        // V√≠ d·ª•: consensus_index 0-9 ‚Üí block 0, 10-19 ‚Üí block 1, 20-29 ‚Üí block 2
        let block_height = consensus_index / BLOCK_SIZE;
        let block_start_index = block_height * BLOCK_SIZE;
        let block_end_index = (block_height + 1) * BLOCK_SIZE - 1;
        
        // CRITICAL: Ki·ªÉm tra block ƒë√£ g·ª≠i ch∆∞a
        let last_sent_guard = self.last_sent_height.lock().await;
        let last_sent = *last_sent_guard;
        drop(last_sent_guard);
        
        if let Some(last_sent_val) = last_sent {
            if block_height <= last_sent_val {
                // Block ƒë√£ g·ª≠i r·ªìi ‚Üí certificate n√†y ƒë·∫øn mu·ªôn (kh√¥ng n√™n x·∫£y ra v·ªõi consensus_index tu·∫ßn t·ª±)
                // PRODUCTION: Buffer late certificate ƒë·ªÉ retry sau
                warn!(
                    "‚ö†Ô∏è [UDS] Late certificate for already-sent block: Round={}, BlockHeight={}, LastSent={}, ConsensusIndex={}, HasTransaction={}. Buffering for retry.",
                    round, block_height, last_sent_val, consensus_index, has_transaction
                );
                
                // Buffer late certificate info ƒë·ªÉ monitoring
                let mut late_certs = self.late_certificates.lock().await;
                
                // Gi·ªõi h·∫°n buffer size ƒë·ªÉ tr√°nh memory leak
                if late_certs.len() >= 1000 {
                    warn!("‚ö†Ô∏è [UDS] Late certificate buffer full ({}). Dropping oldest entries.", late_certs.len());
                    late_certs.drain(0..500); // Remove oldest 500
                }
                
                // L∆∞u th√¥ng tin ƒë·ªÉ monitoring
                late_certs.push((block_height, consensus_index, round, has_transaction));
                
                // V·ªõi consensus_index tu·∫ßn t·ª±, late certificate kh√¥ng n√™n x·∫£y ra
                // N·∫øu x·∫£y ra, c√≥ th·ªÉ l√† bug ho·∫∑c network issue
                // Note: Kh√¥ng th·ªÉ retry v√¨ ConsensusOutput kh√¥ng c√≥ Clone
                error!("‚ùå [UDS] Late certificate detected (cannot retry). This should not happen with sequential consensus_index! Round={}, ConsensusIndex={}, BlockHeight={}, LastSent={}, HasTransaction={}", 
                    round, consensus_index, block_height, last_sent_val, has_transaction);
                
                return;
            }
        }
        
        // T·ªêI ∆ØU: Track batch ƒë·ªÉ detect missed (ch·ªâ track, kh√¥ng retry ph·ª©c t·∫°p)
        // Kh√¥ng blocking consensus - ch·ªâ quick lookup v√† insert
        if let Some(batch_digest) = batch_digest_opt {
            // Quick check: processed_batch_digests (read-only, fast, minimal lock time)
            let is_processed = {
                let processed_batch_guard = self.processed_batch_digests.lock().await;
                processed_batch_guard.contains_key(&batch_digest)
            };
            
            // Quick update: missed_batches (minimal lock time)
            let mut missed_guard = self.missed_batches.lock().await;
            
            if is_processed {
                // Batch ƒë√£ processed ‚Üí remove kh·ªèi missed_batches (fast remove)
                missed_guard.remove(&batch_digest);
            } else {
                // Batch ch∆∞a processed ‚Üí track (lazy GC - ch·ªâ khi cache ƒë·∫ßy)
                const MAX_MISSED_BATCHES: usize = 5000;
                if missed_guard.len() >= MAX_MISSED_BATCHES {
                    // Fast GC: X√≥a 50% entries (kh√¥ng sort - ch·ªâ remove random ƒë·ªÉ tr√°nh overhead)
                    let target_size = MAX_MISSED_BATCHES / 2;
                    let mut removed = 0;
                    let current_size = missed_guard.len();
                    missed_guard.retain(|_, _| {
                        removed += 1;
                        // Remove m·ªói entry th·ª© 2 ƒë·ªÉ gi·∫£m size xu·ªëng target_size
                        removed <= target_size || (removed - target_size) % 2 == 0
                    });
                    if missed_guard.len() < current_size {
                        debug!("üßπ [UDS] GC: Cleaned missed_batches from {} to {} entries", current_size, missed_guard.len());
                    }
                }
                
                // Fast insert
                if !missed_guard.contains_key(&batch_digest) {
                    missed_guard.insert(batch_digest, MissedBatchInfo {
                        commit_time: Instant::now(),
                        consensus_index,
                        round,
                        block_height,
                        retry_count: 0,
                        last_retry_time: Instant::now(),
                    });
                }
            }
        }
        
        // T·ªêI ∆ØU: Check missed batches ch·ªâ khi c·∫ßn (kh√¥ng blocking consensus)
        // Defer check ra kh·ªèi hot path - ch·ªâ check m·ªói 100 certificates ƒë·ªÉ tr√°nh overhead
        // Kh√¥ng spawn task ƒë·ªÉ tr√°nh lifetime issues - ch·ªâ check inline nh∆∞ng nhanh
        if consensus_index % 100 == 0 {
            let missed_guard = self.missed_batches.lock().await;
            if !missed_guard.is_empty() {
                drop(missed_guard);
                // Check missed batches inline (nhanh, kh√¥ng blocking)
                self.check_missed_batches().await;
            }
        }
        
        // Block ch∆∞a g·ª≠i ‚Üí th√™m transaction v√†o block
        let mut current_block_guard = self.current_block.lock().await;
        
        // Ki·ªÉm tra xem c√≥ c·∫ßn t·∫°o block m·ªõi kh√¥ng
        let need_new_block = current_block_guard.is_none() 
            || current_block_guard.as_ref().unwrap().height != block_height;
        
        // L∆∞u block c≈© (n·∫øu c√≥) ƒë·ªÉ g·ª≠i sau khi th√™m transaction v√†o block m·ªõi
        let mut old_block_to_send: Option<BlockBuilder> = None;
        
        if need_new_block {
            // C·∫ßn t·∫°o block m·ªõi ‚Üí l∆∞u block c≈© ƒë·ªÉ g·ª≠i sau
            if let Some(old_block) = current_block_guard.take() {
                old_block_to_send = Some(old_block);
            }
            
            // T·∫°o block m·ªõi cho block_height n√†y
            if has_transaction {
                if tx_count > 1 {
                    info!("üìä [UDS] Creating block {} (consensus_index {}-{}): Round={}, ConsensusIndex={}, TxCount={} (Transactions protobuf)", 
                        block_height, block_start_index, block_end_index, round, consensus_index, tx_count);
                } else if tx_count == 1 {
                    let first_hash = &parsed_transactions[0].0;
                    info!("üìä [UDS] Creating block {} (consensus_index {}-{}): Round={}, ConsensusIndex={}, TxHash={}", 
                        block_height, block_start_index, block_end_index, round, consensus_index, first_hash);
                }
            }
            
            *current_block_guard = Some(BlockBuilder {
                epoch: self.epoch,
                height: block_height,
                transaction_entries: Vec::new(),
                transaction_hashes: HashSet::new(),
            });
        }
        
        // Th√™m T·∫§T C·∫¢ transactions v√†o block (n·∫øu c√≥)
        // CRITICAL: 1 batch ch·ª©a m·ªôt m·∫£ng giao d·ªãch - x·ª≠ l√Ω T·∫§T C·∫¢ transactions
        // Transaction n√†y t·ª´ certificate ƒê√É ƒê∆Ø·ª¢C COMMIT (ConsensusOutput)
        
        // CRITICAL: Log ƒë·ªÉ trace giao d·ªãch c·ª• th·ªÉ khi th√™m v√†o block
        for (tx_hash_hex, _, _, _) in &parsed_transactions {
            if should_trace_tx(tx_hash_hex) {
                info!("üîç [UDS] TRACE: Adding transaction {} to block {} (consensus_index={}, block_height={}, block_start_index={}, block_end_index={})", 
                    tx_hash_hex, block_height, consensus_index, block_height, block_start_index, block_end_index);
            }
        }
        
        if !parsed_transactions.is_empty() {
            let worker_id = consensus_output
                .certificate
                .header
                .payload
                .iter()
                .nth(execution_indices.next_batch_index.saturating_sub(1) as usize)
                .map(|(_, worker_id)| *worker_id)
                .unwrap_or(0u32);
            
            if let Some(block) = current_block_guard.as_mut() {
                // X·ª≠ l√Ω T·∫§T C·∫¢ transactions trong Transactions protobuf
                // M·ªói transaction c√≥ c√πng consensus_index (t·ª´ certificate)
                // CRITICAL: ƒê·∫£m b·∫£o d·ªØ li·ªáu nh·∫•t qu√°n - transaction bytes ph·∫£i gi·ªØ nguy√™n t·ª´ parse ƒë·∫øn g·ª≠i UDS
                for (tx_idx, (tx_hash_hex, tx_hash, _tx_proto, raw_bytes)) in parsed_transactions.iter().enumerate() {
                    // CRITICAL: Check duplicate trong c√πng block
                    // Note: Batch duplicate ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω b·ªüi processed_batch_digests
                    // Transaction duplicate gi·ªØa c√°c blocks ƒë∆∞·ª£c prevent b·ªüi batch-level deduplication
                    // Ch·ªâ c·∫ßn check duplicate trong c√πng block
                    if block.transaction_hashes.contains(tx_hash) {
                        warn!("‚ö†Ô∏è [UDS] Duplicate transaction detected in block {}: TxHash={}, Round={}, ConsensusIndex={}, TxIdx={}/{}. Transaction already exists in block. This transaction will NOT be added to block again.", 
                            block_height, tx_hash_hex, round, consensus_index, tx_idx, tx_count);
                        // CRITICAL: Log ƒë·ªÉ trace giao d·ªãch b·ªã skip
                        if should_trace_tx(tx_hash_hex) {
                            error!("‚ùå [UDS] TRACE: Transaction {} was SKIPPED due to duplicate in block {} (Round={}, ConsensusIndex={})", 
                                tx_hash_hex, block_height, round, consensus_index);
                        }
                        continue;
                    }
                    
                    // Transaction kh√¥ng duplicate trong block ‚Üí th√™m v√†o block
                    // 
                    // CRITICAL: S·ª≠ d·ª•ng raw_bytes TR·ª∞C TI·∫æP (bytes g·ªëc, kh√¥ng serialize l·∫°i)
                    // NOTE: Kh√¥ng check xem raw_bytes c√≥ th·ªÉ parse nh∆∞ Transactions wrapper v√¨:
                    // - Transaction bytes h·ª£p l·ªá c√≥ th·ªÉ v√¥ t√¨nh parse ƒë∆∞·ª£c nh∆∞ wrapper (do protobuf wire format)
                    // - Validation hash ƒë√£ ƒë·∫£m b·∫£o raw_bytes l√† transaction bytes ƒë√∫ng
                    // - raw_bytes l√† transaction bytes G·ªêC (ƒë√£ serialize t·ª´ protobuf object)
                    // - N·∫øu parse ƒë∆∞·ª£c Transactions wrapper: raw_bytes = serialized transaction bytes t·ª´ protobuf object
                    // - N·∫øu parse ƒë∆∞·ª£c single Transaction: raw_bytes = serialized transaction bytes t·ª´ protobuf object
                    //
                    // QUAN TR·ªåNG:
                    // - Hash ƒë∆∞·ª£c t√≠nh t·ª´ protobuf object (calculate_transaction_hash_from_proto)
                    // - Hash KH√îNG ·∫£nh h∆∞·ªüng ƒë·∫øn bytes - ch·ªâ ƒë·ªçc fields ƒë·ªÉ t√≠nh hash
                    // - Bytes ƒë∆∞·ª£c l∆∞u v√†o digest l√† bytes G·ªêC (serialized t·ª´ protobuf object)
                    // - Bytes n√†y s·∫Ω ƒë∆∞·ª£c g·ª≠i NGUY√äN V·∫∏N sang UDS qua protobuf (protobuf ch·ªâ wrap bytes, kh√¥ng thay ƒë·ªïi n·ªôi dung)
                    // - Go side s·∫Ω nh·∫≠n ƒë∆∞·ª£c ƒë√∫ng bytes g·ªëc v√† parse ƒë∆∞·ª£c ‚Üí hash s·∫Ω kh·ªõp
                    let tx_digest_bytes = Bytes::from(raw_bytes.clone());
                    
                    // CRITICAL: Log hex c·ªßa digest bytes ƒë·ªÉ trace (ch·ªâ cho transaction ƒë∆∞·ª£c trace)
                    if should_trace_tx(tx_hash_hex) {
                        let digest_hex = hex::encode(raw_bytes);
                        info!("üîç [UDS] TRACE: Digest bytes hex for {} when adding to block: {} (first 100 chars: {})", 
                            tx_hash_hex,
                            if digest_hex.len() > 200 { format!("{}...", &digest_hex[..200]) } else { digest_hex.clone() },
                            if digest_hex.len() > 100 { &digest_hex[..100] } else { &digest_hex });
                    }
                    
                    // VALIDATION: ƒê·∫£m b·∫£o hash t√≠nh t·ª´ raw_bytes kh·ªõp v·ªõi hash ƒë√£ l∆∞u
                    // CRITICAL: raw_bytes l√† transaction bytes ƒë√£ extract (KH√îNG ph·∫£i wrapper)
                    // ‚Üí Ch·ªâ parse nh∆∞ Transaction (single), KH√îNG parse nh∆∞ Transactions wrapper
                    // ‚Üí N·∫øu parse nh∆∞ wrapper ‚Üí s·∫Ω extract l·∫°i ‚Üí hash sai
                    
                    // CRITICAL: Log hex c·ªßa raw_bytes ƒë·ªÉ trace (ch·ªâ cho transaction ƒë∆∞·ª£c trace)
                    if should_trace_tx(tx_hash_hex) {
                        let raw_bytes_hex = hex::encode(raw_bytes);
                        info!("üîç [UDS] TRACE: Raw bytes hex for {} BEFORE adding to block: {} (first 100 chars: {})", 
                            tx_hash_hex,
                            if raw_bytes_hex.len() > 200 { format!("{}...", &raw_bytes_hex[..200]) } else { raw_bytes_hex.clone() },
                            if raw_bytes_hex.len() > 100 { &raw_bytes_hex[..100] } else { &raw_bytes_hex });
                    }
                    
                    match transaction::Transaction::decode(raw_bytes.as_slice()) {
                        Ok(parsed_tx) => {
                            let validation_hash = calculate_transaction_hash_from_proto(&parsed_tx);
                            let validation_hash_hex = hex::encode(&validation_hash);
                            if validation_hash_hex != *tx_hash_hex {
                                error!("‚ùå [UDS] CRITICAL: Hash mismatch for transaction! Stored hash: {}, Calculated from raw_bytes: {}, RawBytesLen: {}", 
                                    tx_hash_hex, validation_hash_hex, raw_bytes.len());
                                
                                // CRITICAL: Log hex c·ªßa raw_bytes khi hash mismatch
                                if should_trace_tx(tx_hash_hex) {
                                    let raw_bytes_hex = hex::encode(raw_bytes);
                                    error!("‚ùå [UDS] TRACE: Raw bytes hex when hash mismatch for {}: {} (full: {})", 
                                        tx_hash_hex,
                                        if raw_bytes_hex.len() > 200 { format!("{}...", &raw_bytes_hex[..200]) } else { raw_bytes_hex.clone() },
                                        raw_bytes_hex);
                                }
                                
                                panic!("CRITICAL: Hash validation failed - transaction bytes corrupted!");
                            } else {
                                debug!("‚úÖ [UDS] Hash validation passed: TxHash={}, BytesLen={}", tx_hash_hex, raw_bytes.len());
                            }
                        }
                        Err(e) => {
                            error!("‚ùå [UDS] CRITICAL: Cannot parse raw_bytes as Transaction for validation! RawBytesLen: {}, Error: {:?}", 
                                raw_bytes.len(), e);
                            
                            // CRITICAL: Log hex c·ªßa raw_bytes khi parse failed
                            if should_trace_tx(tx_hash_hex) {
                                let raw_bytes_hex = hex::encode(raw_bytes);
                                error!("‚ùå [UDS] TRACE: Raw bytes hex when parse failed for {}: {} (full: {})", 
                                    tx_hash_hex,
                                    if raw_bytes_hex.len() > 200 { format!("{}...", &raw_bytes_hex[..200]) } else { raw_bytes_hex.clone() },
                                    raw_bytes_hex);
                            }
                            
                            panic!("CRITICAL: Cannot validate transaction bytes - parsing failed!");
                        }
                    }
                    
                    block.transaction_entries.push(TransactionEntry {
                        consensus_index,
                        transaction: comm::Transaction {
                            digest: tx_digest_bytes,
                            worker_id,
                        },
                        tx_hash_hex: tx_hash_hex.clone(), // L∆∞u hash ƒë·ªÉ d√πng khi finalize
                        batch_digest: batch_digest_opt, // L∆∞u batch_digest ƒë·ªÉ check khi retry
                    });
                    block.transaction_hashes.insert(tx_hash.clone());
                    
                    // CRITICAL: Log ƒë·ªÉ trace giao d·ªãch ƒë∆∞·ª£c th√™m v√†o block
                    if should_trace_tx(tx_hash_hex) {
                        info!("‚úÖ [UDS] TRACE: Transaction {} was ADDED to block {} (Round={}, ConsensusIndex={}, TotalTxs={})", 
                            tx_hash_hex, block_height, round, consensus_index, block.transaction_entries.len());
                    }
                    
                    if tx_count > 1 {
                        info!("üìù [UDS] Added transaction [{}/{}] to block {}: Round={}, ConsensusIndex={}, TxHash={}, TotalTxs={}, WorkerId={}", 
                            tx_idx + 1, tx_count, block_height, round, consensus_index, tx_hash_hex, block.transaction_entries.len(), worker_id);
                    } else {
                        info!("üìù [UDS] Added transaction to block {}: Round={}, ConsensusIndex={}, TxHash={}, TotalTxs={}, WorkerId={}", 
                            block_height, round, consensus_index, tx_hash_hex, block.transaction_entries.len(), worker_id);
                    }
                }
            } else {
                // DEFENSIVE: Block kh√¥ng t·ªìn t·∫°i (kh√¥ng n√™n x·∫£y ra)
                // ƒêi·ªÅu n√†y c√≥ th·ªÉ x·∫£y ra n·∫øu logic t·∫°o block c√≥ bug
                warn!("‚ö†Ô∏è [UDS] CRITICAL: Block kh√¥ng t·ªìn t·∫°i khi x·ª≠ l√Ω transaction! BlockHeight={}, ConsensusIndex={}, Round={}, HasTransaction={}. ƒê√¢y c√≥ th·ªÉ l√† bug!", 
                    block_height, consensus_index, round, has_transaction);
            }
        }
        
        // CRITICAL: Mark batch as processed SAU KHI ƒë√£ th√™m transactions v√†o block
        // 
        // V·∫§N ƒê·ªÄ: Notifier g·ªçi handle_consensus_transaction cho M·ªñI transaction trong batch
        // - T·∫•t c·∫£ transactions trong c√πng batch c√≥ c√πng batch_digest v√† consensus_index
        // - Batch duplicate ƒë√£ ƒë∆∞·ª£c check TR∆Ø·ªöC KHI th√™m transactions ‚Üí ƒë·∫£m b·∫£o kh√¥ng th√™m duplicate batch
        //
        // GI·∫¢I PH√ÅP:
        // - Track (batch_digest, consensus_index) ƒë·ªÉ bi·∫øt batch ƒë√£ ƒë∆∞·ª£c processed v·ªõi consensus_index n√†o
        // - N·∫øu batch ch∆∞a c√≥ trong map ‚Üí l∆∞u (batch_digest, consensus_index) sau khi x·ª≠ l√Ω xong
        // - N·∫øu batch ƒë√£ c√≥ trong map v·ªõi c√πng consensus_index ‚Üí ƒë√£ ƒë∆∞·ª£c check tr∆∞·ªõc ƒë√≥, kh√¥ng c·∫ßn l√†m g√¨
        // - Transaction_hashes trong BlockBuilder prevent duplicate trong c√πng block ‚Üí an to√†n
        //
        // FORK-SAFE: T·∫•t c·∫£ nodes track c√πng batches ‚Üí c√πng quy·∫øt ƒë·ªãnh skip ‚Üí fork-safe
        if let Some(batch_digest) = batch_digest_opt {
            let mut processed_batch_guard = self.processed_batch_digests.lock().await;
            
            // Check xem batch ƒë√£ ƒë∆∞·ª£c processed ch∆∞a
            if !processed_batch_guard.contains_key(&batch_digest) {
                // Batch ch∆∞a ƒë∆∞·ª£c processed ‚Üí l∆∞u (batch_digest, consensus_index) sau khi x·ª≠ l√Ω xong
                processed_batch_guard.insert(batch_digest, consensus_index);
                info!(
                    "‚úÖ [UDS] Marked batch as processed: BatchDigest={:?}, ConsensusIndex={}, Round={}, TxCount={}",
                    batch_digest, consensus_index, round, tx_count
                );
                // Batch ƒë√£ processed ‚Üí cleanup
                drop(processed_batch_guard);
                
                // Remove kh·ªèi missed_batches
                let mut missed_guard = self.missed_batches.lock().await;
                missed_guard.remove(&batch_digest);
                drop(missed_guard);
                
                // Remove kh·ªèi logged_duplicate_batches
                let mut logged_guard = self.logged_duplicate_batches.lock().await;
                logged_guard.remove(&batch_digest);
                drop(logged_guard);
                
                processed_batch_guard = self.processed_batch_digests.lock().await;
                
                // FORK-SAFE: GC - cleanup entries c≈© (gi·ªõi h·∫°n cache)
                // CRITICAL: GC ph·∫£i deterministic d·ª±a tr√™n consensus_index ƒë·ªÉ ƒë·∫£m b·∫£o t·∫•t c·∫£ nodes c√≥ c√πng state
                // T·∫•t c·∫£ nodes v·ªõi c√πng consensus_index s·∫Ω x√≥a c√πng entries ‚Üí fork-safe
                const MAX_PROCESSED_BATCHES: usize = 10000;
                if processed_batch_guard.len() > MAX_PROCESSED_BATCHES {
                    // FORK-SAFE: X√≥a entries c≈© nh·∫•t d·ª±a tr√™n consensus_index (deterministic)
                    // gc_threshold = consensus_index - GC_DEPTH * BLOCK_SIZE
                    // T·∫•t c·∫£ nodes v·ªõi c√πng consensus_index s·∫Ω c√≥ c√πng gc_threshold ‚Üí x√≥a c√πng entries
                    let gc_threshold = consensus_index.saturating_sub(GC_DEPTH * BLOCK_SIZE);
                    if gc_threshold > 0 {
                        let before_size = processed_batch_guard.len();
                        processed_batch_guard.retain(|_, stored_index| *stored_index >= gc_threshold);
                        let after_size = processed_batch_guard.len();
                        let cleaned = before_size.saturating_sub(after_size);
                        if cleaned > 0 {
                            debug!("üßπ [UDS] GC: Cleaned {} old batch entries (threshold: {}, before: {}, after: {})", 
                                cleaned, gc_threshold, before_size, after_size);
                        }
                    }
                }
            } else {
                // Batch ƒë√£ ƒë∆∞·ª£c processed v·ªõi c√πng consensus_index ‚Üí transaction ti·∫øp theo trong batch
                // ƒê√£ ƒë∆∞·ª£c check tr∆∞·ªõc khi th√™m transactions ‚Üí transaction ƒë√£ ƒë∆∞·ª£c th√™m v√†o block
                debug!(
                    "üîç [UDS] Batch already processed with same consensus_index: BatchDigest={:?}, ConsensusIndex={}, Round={}, TxCount={}. Transaction continuation in batch (already added to block).",
                    batch_digest, consensus_index, round, tx_count
                );
            }
            drop(processed_batch_guard);
        }
        
        drop(current_block_guard);
        
        // G·ª≠i block c≈© SAU KHI ƒë√£ th√™m transaction v√†o block m·ªõi
        // ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o block c≈© c√≥ ƒë·∫ßy ƒë·ªß transactions tr∆∞·ªõc khi g·ª≠i
        if let Some(old_block) = old_block_to_send {
            let old_block_height = old_block.height;
            let old_block_tx_count = old_block.transaction_entries.len();
            info!("üì§ [UDS] Switching to new block {}: Sending previous block {} with {} transactions (after adding transaction to new block)", 
                block_height, old_block_height, old_block_tx_count);
            
            // CRITICAL: Log ƒë·ªÉ trace giao d·ªãch trong block c≈©
            for entry in &old_block.transaction_entries {
                if should_trace_tx(&entry.tx_hash_hex) {
                    info!("‚úÖ [UDS] TRACE: Transaction {} is in OLD block {} being sent (Round={}, ConsensusIndex={})", 
                        entry.tx_hash_hex, old_block_height, round, entry.consensus_index);
                }
            }
            
            // G·ª≠i block c≈© tr·ª±c ti·∫øp (kh√¥ng c·∫ßn l·∫•y t·ª´ current_block v√¨ ƒë√£ l·∫•y r·ªìi)
            let (block_to_send, tx_hash_map, batch_digests) = old_block.finalize();
            
            // FORK-SAFE: Atomic check-and-send
            // CRITICAL: T·∫•t c·∫£ nodes check c√πng last_sent_height ‚Üí c√πng quy·∫øt ƒë·ªãnh g·ª≠i block ‚Üí fork-safe
            // Logic: Ch·ªâ g·ª≠i n·∫øu block.height > last_sent_height (ho·∫∑c last_sent_height = None)
            // T·∫•t c·∫£ nodes v·ªõi c√πng consensus_index s·∫Ω c√≥ c√πng last_sent_height ‚Üí c√πng quy·∫øt ƒë·ªãnh
            let mut last_sent_guard = self.last_sent_height.lock().await;
            let should_send = last_sent_guard.is_none() || 
                block_to_send.height > last_sent_guard.unwrap();
            
            if should_send {
                // Fill gaps tr∆∞·ªõc khi g·ª≠i block
                if let Some(last_sent_val) = *last_sent_guard {
                    if block_to_send.height > last_sent_val + 1 {
                        drop(last_sent_guard);
                        if let Err(e) = self.send_empty_blocks_for_gaps(last_sent_val + 1, block_to_send.height).await {
                            error!("‚ùå [UDS] Failed to send empty blocks for gaps: {}", e);
                        }
                        last_sent_guard = self.last_sent_height.lock().await;
                    }
                } else {
                    // Ch∆∞a g·ª≠i block n√†o ‚Üí fill t·ª´ 0
                    if block_to_send.height > 0 {
                        drop(last_sent_guard);
                        if let Err(e) = self.send_empty_blocks_for_gaps(0, block_to_send.height).await {
                            error!("‚ùå [UDS] Failed to send empty blocks for gaps: {}", e);
                        }
                        last_sent_guard = self.last_sent_height.lock().await;
                    }
                }
                
                // Final check
                let final_should_send = last_sent_guard.is_none() || 
                    block_to_send.height > last_sent_guard.unwrap();
                
                if final_should_send {
                    drop(last_sent_guard);
                    
                    // CRITICAL: G·ª≠i block TR∆Ø·ªöC, sau ƒë√≥ m·ªõi log "executed"
                    // Ch·ªâ log "executed" SAU KHI block ƒë∆∞·ª£c g·ª≠i th√†nh c√¥ng
                    info!("üì§ [UDS] Attempting to send block {} with {} transactions", 
                        old_block_height, block_to_send.transactions.len());
                    if let Err(e) = self.send_block_with_retry(block_to_send.clone(), tx_hash_map.clone(), batch_digests.clone()).await {
                        error!("‚ùå [UDS] Failed to send block height {} after retries: {}", old_block_height, e);
                        // CRITICAL: Log ƒë·ªÉ trace n·∫øu block ch·ª©a giao d·ªãch ƒë∆∞·ª£c trace
                        for tx in &block_to_send.transactions {
                            let digest_key = tx.digest.as_ref().to_vec();
                            if let Some(tx_hash_hex) = tx_hash_map.get(&digest_key) {
                                if should_trace_tx(tx_hash_hex) {
                                    error!("‚ùå [UDS] TRACE: Transaction {} FAILED to send in block {}: {}", 
                                        tx_hash_hex, old_block_height, e);
                                }
                            }
                        }
                    } else {
                        info!("‚úÖ [UDS] Successfully sent block {} with {} transactions", 
                            old_block_height, block_to_send.transactions.len());
                        // CRITICAL: Log ƒë·ªÉ trace n·∫øu block ch·ª©a giao d·ªãch ƒë∆∞·ª£c trace
                        for tx in &block_to_send.transactions {
                            let digest_key = tx.digest.as_ref().to_vec();
                            if let Some(tx_hash_hex) = tx_hash_map.get(&digest_key) {
                                if should_trace_tx(tx_hash_hex) {
                                    info!("‚úÖ [UDS] TRACE: Transaction {} was successfully sent in block {}", 
                                        tx_hash_hex, old_block_height);
                                }
                            }
                        }
                        // Block ƒë∆∞·ª£c g·ª≠i th√†nh c√¥ng ‚Üí log "executed"
                        if !block_to_send.transactions.is_empty() {
                            info!("‚úÖ [UDS] Block {} sent successfully with {} transactions", 
                                block_to_send.height, block_to_send.transactions.len());
                            
                            // Log transaction hashes ƒë·ªÉ trace (d√πng hash ƒë√£ l∆∞u s·∫µn - ƒë·∫£m b·∫£o nh·∫•t qu√°n)
                            for (idx, tx) in block_to_send.transactions.iter().enumerate() {
                                let digest_key = tx.digest.as_ref().to_vec();
                                if let Some(tx_hash_hex) = tx_hash_map.get(&digest_key) {
                                    info!("  ‚úÖ [UDS] Block {} Tx[{}] executed: TxHash={}, WorkerId={}", 
                                        block_to_send.height, idx, tx_hash_hex, tx.worker_id);
                                } else {
                                    error!("  ‚ùå [UDS] Block {} Tx[{}] executed but hash not found in map: WorkerId={}", 
                                        block_to_send.height, idx, tx.worker_id);
                                }
                            }
                        }
                        
                        // FORK-SAFE: Atomic update last_sent_height
                        // CRITICAL: Ch·ªâ update khi block ƒë∆∞·ª£c g·ª≠i th√†nh c√¥ng
                        // T·∫•t c·∫£ nodes v·ªõi c√πng consensus_index s·∫Ω g·ª≠i c√πng blocks ‚Üí c√πng update last_sent_height ‚Üí fork-safe
                        let mut last_sent_guard = self.last_sent_height.lock().await;
                        let should_update = last_sent_guard.is_none() || 
                            block_to_send.height > last_sent_guard.unwrap();
                        if should_update {
                            *last_sent_guard = Some(block_to_send.height);
                        }
                    }
                }
            }
        }
        
        // CRITICAL: G·ª≠i block CH·ªà KHI c√≥ certificate t·ª´ block ti·∫øp theo
        // - KH√îNG g·ª≠i khi ch·ªâ ƒë·∫°t block_end_index v√¨ batch c√≥ th·ªÉ c√≥ nhi·ªÅu transactions
        // - C√°c transactions trong c√πng batch (c√πng consensus_index) ƒë·∫øn tu·∫ßn t·ª± (async)
        // - N·∫øu g·ª≠i block ngay khi consensus_index >= block_end_index, transaction th·ª© 2, 3... c√≥ th·ªÉ ƒë·∫øn mu·ªôn
        // - CH·ªà g·ª≠i khi consensus_index >= next_block_start_index ƒë·ªÉ ƒë·∫£m b·∫£o T·∫§T C·∫¢ transactions t·ª´ batch ƒë√£ ƒë·∫øn
        // 
        // V√≠ d·ª•: Block 247 (consensus_index 2470-2479)
        // - N·∫øu g·ª≠i khi consensus_index = 2479 ‚Üí transaction th·ª© 2 t·ª´ batch c√≥ th·ªÉ ƒë·∫øn mu·ªôn (v·∫´n consensus_index = 2479)
        // - CH·ªà g·ª≠i khi consensus_index >= 2480 (certificate t·ª´ block 248) ‚Üí ƒë·∫£m b·∫£o t·∫•t c·∫£ transactions t·ª´ block 247 ƒë√£ ƒë·∫øn
        let next_block_start_index = (block_height + 1) * BLOCK_SIZE;
        
        // Debug: Log c√°c gi√° tr·ªã ƒë·ªÉ ki·ªÉm tra
        debug!("üîç [UDS] Block send check: BlockHeight={}, ConsensusIndex={}, BlockStartIndex={}, BlockEndIndex={}, NextBlockStartIndex={}, HasTransaction={}", 
            block_height, consensus_index, block_start_index, block_end_index, next_block_start_index, has_transaction);
        
        // CRITICAL: G·ª≠i block hi·ªán t·∫°i n·∫øu consensus_index ƒë√£ v∆∞·ª£t qu√° block_end_index
        // ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o block ƒë∆∞·ª£c g·ª≠i ngay c·∫£ khi kh√¥ng c√≥ block m·ªõi ti·∫øp theo
        // Logic: N·∫øu consensus_index >= next_block_start_index, block hi·ªán t·∫°i (block_height - 1) n√™n ƒë∆∞·ª£c g·ª≠i
        // Nh∆∞ng n·∫øu need_new_block = true, block c≈© ƒë√£ ƒë∆∞·ª£c g·ª≠i trong logic tr√™n r·ªìi
        // Ch·ªâ c·∫ßn ki·ªÉm tra v√† g·ª≠i block hi·ªán t·∫°i n·∫øu n√≥ ch∆∞a ƒë∆∞·ª£c g·ª≠i
        if consensus_index >= next_block_start_index {
            // consensus_index ƒë√£ v∆∞·ª£t qu√° block hi·ªán t·∫°i ‚Üí c·∫ßn ki·ªÉm tra block hi·ªán t·∫°i c√≥ c·∫ßn g·ª≠i kh√¥ng
            let mut current_block_guard = self.current_block.lock().await;
            if let Some(block) = current_block_guard.as_ref() {
                // Block hi·ªán t·∫°i c√≥ th·ªÉ l√† block_height ho·∫∑c block c≈© h∆°n
                // N·∫øu block.height < block_height, ƒë√¢y l√† block c≈© ch∆∞a ƒë∆∞·ª£c g·ª≠i
                // N·∫øu block.height == block_height, ƒë√¢y l√† block hi·ªán t·∫°i (s·∫Ω ƒë∆∞·ª£c g·ª≠i khi c√≥ block m·ªõi)
                let last_sent_guard = self.last_sent_height.lock().await;
                let last_sent = *last_sent_guard;
                drop(last_sent_guard);
                
                // CRITICAL: Log ƒë·ªÉ debug v·∫•n ƒë·ªÅ "c·ª© 20 giao d·ªãch l√† b·ªã ƒë·ª©ng"
                info!("üîç [UDS] DEBUG: consensus_index {} >= next_block_start_index {}, block.height={}, block_height={}, last_sent={:?}", 
                    consensus_index, next_block_start_index, block.height, block_height, last_sent);
                
                // Ch·ªâ g·ª≠i n·∫øu block ch∆∞a ƒë∆∞·ª£c g·ª≠i
                let should_send_block = if let Some(last_sent_val) = last_sent {
                    block.height > last_sent_val
                } else {
                    true // Ch∆∞a g·ª≠i block n√†o
                };
                
                // CRITICAL: G·ª≠i block n·∫øu:
                // 1. block.height < block_height (block c≈© ch∆∞a ƒë∆∞·ª£c g·ª≠i)
                // 2. HO·∫∂C block.height == block_height nh∆∞ng consensus_index ƒë√£ v∆∞·ª£t qu√° block_end_index
                let block_end_index_for_current = (block.height + 1) * BLOCK_SIZE - 1;
                let should_send = should_send_block && (
                    block.height < block_height || 
                    (block.height == block_height && consensus_index > block_end_index_for_current)
                );
                
                if should_send {
                    // Block c·∫ßn ƒë∆∞·ª£c g·ª≠i ‚Üí g·ª≠i ngay
                    let old_block = current_block_guard.take().unwrap();
                    drop(current_block_guard);
                    
                    let old_block_height = old_block.height;
                    let old_block_tx_count = old_block.transaction_entries.len();
                    info!("üì§ [UDS] Sending pending block {} with {} transactions (consensus_index {} >= next_block_start_index {}, block_height={}, block_end_index={})", 
                        old_block_height, old_block_tx_count, consensus_index, next_block_start_index, block_height, block_end_index_for_current);
                    
                    // CRITICAL: Log ƒë·ªÉ trace giao d·ªãch trong block
                    for entry in &old_block.transaction_entries {
                        if should_trace_tx(&entry.tx_hash_hex) {
                            info!("‚úÖ [UDS] TRACE: Transaction {} is in PENDING block {} being sent (consensus_index={}, entry.consensus_index={})", 
                                entry.tx_hash_hex, old_block_height, consensus_index, entry.consensus_index);
                        }
                    }
                    
                    let (block_to_send, tx_hash_map, batch_digests) = old_block.finalize();
                    
                    // Atomic check-and-send
                    let mut last_sent_guard = self.last_sent_height.lock().await;
                    let final_should_send = last_sent_guard.is_none() || 
                        block_to_send.height > last_sent_guard.unwrap();
                    
                    if final_should_send {
                        drop(last_sent_guard);
                        if let Err(e) = self.send_block_with_retry(block_to_send.clone(), tx_hash_map.clone(), batch_digests.clone()).await {
                            error!("‚ùå [UDS] Failed to send pending block {} after retries: {}", old_block_height, e);
                            // CRITICAL: Log ƒë·ªÉ trace n·∫øu block ch·ª©a giao d·ªãch ƒë∆∞·ª£c trace
                            for tx in &block_to_send.transactions {
                                let digest_key = tx.digest.as_ref().to_vec();
                                if let Some(tx_hash_hex) = tx_hash_map.get(&digest_key) {
                                    if should_trace_tx(tx_hash_hex) {
                                        error!("‚ùå [UDS] TRACE: Transaction {} FAILED to send in pending block {}: {}", 
                                            tx_hash_hex, old_block_height, e);
                                    }
                                }
                            }
                        } else {
                            info!("‚úÖ [UDS] Successfully sent pending block {} with {} transactions", 
                                old_block_height, block_to_send.transactions.len());
                            // CRITICAL: Log ƒë·ªÉ trace n·∫øu block ch·ª©a giao d·ªãch ƒë∆∞·ª£c trace
                            for tx in &block_to_send.transactions {
                                let digest_key = tx.digest.as_ref().to_vec();
                                if let Some(tx_hash_hex) = tx_hash_map.get(&digest_key) {
                                    if should_trace_tx(tx_hash_hex) {
                                        info!("‚úÖ [UDS] TRACE: Transaction {} was successfully sent in pending block {}", 
                                            tx_hash_hex, old_block_height);
                                    }
                                }
                            }
                            
                            // FORK-SAFE: Atomic update last_sent_height
                            // CRITICAL: Ch·ªâ update khi block ƒë∆∞·ª£c g·ª≠i th√†nh c√¥ng
                            // T·∫•t c·∫£ nodes v·ªõi c√πng consensus_index s·∫Ω g·ª≠i c√πng blocks ‚Üí c√πng update last_sent_height ‚Üí fork-safe
                            let mut last_sent_guard = self.last_sent_height.lock().await;
                            let should_update = last_sent_guard.is_none() || 
                                block_to_send.height > last_sent_guard.unwrap();
                            if should_update {
                                *last_sent_guard = Some(block_to_send.height);
                            }
                            drop(last_sent_guard);
                            
                            // Persist state after sending block successfully
                            if let Err(e) = self.persist_execution_state().await {
                                warn!("‚ö†Ô∏è [UDS] Failed to persist execution state after sending block {}: {}", block_to_send.height, e);
                            }
                        }
                    } else {
                        warn!("‚ö†Ô∏è [UDS] Block {} already sent (last_sent_height check), skipping", old_block_height);
                    }
                } else {
                    debug!("‚è≥ [UDS] Block {} not ready to send yet (should_send_block={}, block.height={}, block_height={}, consensus_index={}, block_end_index={})", 
                        block.height, should_send_block, block.height, block_height, consensus_index, block_end_index_for_current);
                }
            } else {
                warn!("‚ö†Ô∏è [UDS] No current block when consensus_index {} >= next_block_start_index {}", 
                    consensus_index, next_block_start_index);
            }
        } else if has_transaction {
            // Log ƒë·ªÉ debug: Block c√≥ transaction nh∆∞ng ch∆∞a ƒë∆∞·ª£c g·ª≠i (ƒë·ª£i certificate t·ª´ block ti·∫øp theo)
            debug!("‚è≥ [UDS] Block {} has transaction (ConsensusIndex={}) but waiting for certificate from next block (BlockEndIndex={}, NextBlockStartIndex={})", 
                block_height, consensus_index, block_end_index, next_block_start_index);
        }
        
        // CRITICAL: X·ª≠ l√Ω c√°c blocks tr∆∞·ªõc ƒë√≥ ch∆∞a ƒë∆∞·ª£c g·ª≠i (n·∫øu c√≥)
        // ƒêi·ªÅu n√†y c√≥ th·ªÉ x·∫£y ra n·∫øu consensus_index tƒÉng nhanh (nh·∫£y qua nhi·ªÅu blocks)
        // V√≠ d·ª•: consensus_index nh·∫£y t·ª´ 5 ‚Üí 15 (nh·∫£y qua block 0 v√† b·∫Øt ƒë·∫ßu block 1)
        // ‚Üí C·∫ßn g·ª≠i block 0 tr∆∞·ªõc
        let last_sent_guard = self.last_sent_height.lock().await;
        let last_sent = *last_sent_guard;
        drop(last_sent_guard);
        
        // G·ª≠i c√°c blocks c√≤n thi·∫øu (fill gaps)
        if let Some(last_sent_val) = last_sent {
            for h in (last_sent_val + 1)..block_height {
                if let Err(e) = self.send_empty_block(h).await {
                    error!("‚ùå [UDS] Failed to send empty block {}: {}", h, e);
                }
            }
        } else {
            // Ch∆∞a g·ª≠i block n√†o ‚Üí fill t·ª´ 0 ƒë·∫øn block_height
            for h in 0..block_height {
                if let Err(e) = self.send_empty_block(h).await {
                    error!("‚ùå [UDS] Failed to send empty block {}: {}", h, e);
                }
            }
        }
        
        // CRITICAL: Ki·ªÉm tra v√† g·ª≠i block hi·ªán t·∫°i n·∫øu c·∫ßn
        // ƒê·∫£m b·∫£o block hi·ªán t·∫°i ƒë∆∞·ª£c g·ª≠i khi consensus_index ƒë√£ v∆∞·ª£t qu√° block_end_index
        // ƒêi·ªÅu n√†y gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ "c·ª© 20 giao d·ªãch l√† b·ªã ƒë·ª©ng"
        self.flush_current_block_if_needed(consensus_index).await;
    }

    async fn load_execution_indices(&self) -> ExecutionIndices {
        let last_consensus_index = {
            let guard = self.last_consensus_index.lock().await;
            *guard
        };
        
        ExecutionIndices {
            next_certificate_index: last_consensus_index + 1,
            next_batch_index: 0,
            next_transaction_index: 0,
        }
    }
}

impl UdsExecutionState {
    /// Flush block hi·ªán t·∫°i n·∫øu c·∫ßn thi·∫øt
    /// G·ª≠i block hi·ªán t·∫°i khi consensus_index ƒë√£ v∆∞·ª£t qu√° block_end_index
    /// ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o block ƒë∆∞·ª£c g·ª≠i ngay c·∫£ khi kh√¥ng c√≥ certificate t·ª´ block ti·∫øp theo
    /// 
    /// OPTIMIZED: Minimize lock scope ƒë·ªÉ gi·∫£m contention
    async fn flush_current_block_if_needed(&self, consensus_index: u64) {
        // OPTIMIZATION: Quick check v·ªõi minimal lock time
        let (block_height, block_tx_count, should_flush) = {
            let current_block_guard = self.current_block.lock().await;
            if let Some(block) = current_block_guard.as_ref() {
                let block_end_index = (block.height + 1) * BLOCK_SIZE - 1;
                let should_flush = consensus_index > block_end_index;
                if should_flush {
                    (Some(block.height), Some(block.transaction_entries.len()), true)
                } else {
                    (None, None, false)
                }
            } else {
                (None, None, false)
            }
        };
        
        if !should_flush {
            return;
        }
        
        let block_height = block_height.unwrap();
        let block_tx_count = block_tx_count.unwrap();
        let block_end_index = (block_height + 1) * BLOCK_SIZE - 1;
        
        // OPTIMIZATION: Check last_sent_height tr∆∞·ªõc khi lock current_block l√¢u
        let last_sent = {
            let last_sent_guard = self.last_sent_height.lock().await;
            *last_sent_guard
        };
        
        let should_send = if let Some(last_sent_val) = last_sent {
            block_height > last_sent_val
        } else {
            true
        };
        
        if !should_send {
            return;
        }
        
        info!("üì§ [UDS] Flushing block {} with {} transactions (consensus_index {} > block_end_index {})", 
            block_height, block_tx_count, consensus_index, block_end_index);
        
        // OPTIMIZATION: Lock current_block ch·ªâ khi c·∫ßn take block
        let (block_to_send, tx_hash_map, batch_digests, trace_hashes) = {
            let mut current_block_guard = self.current_block.lock().await;
            if let Some(block) = current_block_guard.as_ref() {
                // Collect trace hashes tr∆∞·ªõc khi take block
                let trace_hashes: Vec<String> = block.transaction_entries.iter()
                    .filter(|e| should_trace_tx(&e.tx_hash_hex))
                    .map(|e| e.tx_hash_hex.clone())
                    .collect();
                
                let old_block = current_block_guard.take().unwrap();
                drop(current_block_guard);
                
                let (block_to_send, tx_hash_map, batch_digests) = old_block.finalize();
                (block_to_send, tx_hash_map, batch_digests, trace_hashes)
            } else {
                return; // Block ƒë√£ ƒë∆∞·ª£c take b·ªüi thread kh√°c
            }
        };
        
        // Log trace hashes (kh√¥ng c·∫ßn lock)
        for tx_hash_hex in &trace_hashes {
            info!("‚úÖ [UDS] TRACE: Transaction {} is in FLUSHED block {} being sent", 
                tx_hash_hex, block_height);
        }
        
        // Atomic check-and-send
        let final_should_send = {
            let mut last_sent_guard = self.last_sent_height.lock().await;
            let should_send = last_sent_guard.is_none() || 
                block_to_send.height > last_sent_guard.unwrap();
            if should_send {
                *last_sent_guard = Some(block_to_send.height);
            }
            should_send
        };
        
        if final_should_send {
            // OPTIMIZATION: S·ª≠ d·ª•ng trace_hashes ƒë√£ collect tr∆∞·ªõc ƒë√≥ thay v√¨ loop l·∫°i
            match self.send_block_with_retry(block_to_send.clone(), tx_hash_map.clone(), batch_digests.clone()).await {
                Err(e) => {
                    error!("‚ùå [UDS] Failed to flush block {} after retries: {}", block_height, e);
                    // Log trace hashes n·∫øu c√≥
                    for tx_hash_hex in &trace_hashes {
                        error!("‚ùå [UDS] TRACE: Transaction {} FAILED to send in flushed block {}: {}", 
                            tx_hash_hex, block_height, e);
                    }
                }
                Ok(_) => {
                    info!("‚úÖ [UDS] Successfully flushed block {} with {} transactions", 
                        block_height, block_to_send.transactions.len());
                    // Log trace hashes n·∫øu c√≥
                    for tx_hash_hex in &trace_hashes {
                        info!("‚úÖ [UDS] TRACE: Transaction {} was successfully sent in flushed block {}", 
                            tx_hash_hex, block_height);
                    }
                }
            }
        }
    }
}

impl UdsExecutionState {
    /// T·ªêI ∆ØU: Check v√† log missed batches (kh√¥ng retry ph·ª©c t·∫°p)
    /// Ch·ªâ ph√°t hi·ªán v√† log, kh√¥ng ·∫£nh h∆∞·ªüng consensus
    /// Kh√¥ng blocking - ch·ªâ quick operations
    async fn check_missed_batches(&self) {
        let now = Instant::now();
        let timeout_duration = Duration::from_millis(self.missed_batch_timeout_ms);
        
        // T·ªêI ∆ØU: Collect batches c·∫ßn x·ª≠ l√Ω tr∆∞·ªõc ƒë·ªÉ tr√°nh nested locks v√† borrow conflict
        // Minimize lock time - ch·ªâ lock m·ªôt l·∫ßn ƒë·ªÉ collect data
        let mut to_remove: Vec<BatchDigest> = Vec::new();
        let mut to_log: Vec<(BatchDigest, u64, u64, u64)> = Vec::new(); // (digest, consensus_index, round, block_height)
        
        // Quick snapshot: Collect data v·ªõi minimal lock time
        {
            let missed_guard = self.missed_batches.lock().await;
            // Quick check: processed_batch_digests (read-only, fast)
            let processed_guard = self.processed_batch_digests.lock().await;
            
            for (batch_digest, info) in missed_guard.iter() {
                // Check xem batch ƒë√£ ƒë∆∞·ª£c processed ch∆∞a (fast lookup)
                if processed_guard.contains_key(batch_digest) {
                    to_remove.push(*batch_digest);
                    continue;
                }
                
                // Check xem batch c√≥ b·ªã missed kh√¥ng (ƒë√£ qu√° timeout)
                let elapsed = now.duration_since(info.commit_time);
                if elapsed >= timeout_duration && info.retry_count == 0 {
                    // Batch b·ªã missed ‚Üí s·∫Ω log sau
                    to_log.push((*batch_digest, info.consensus_index, info.round, info.block_height));
                }
            }
            drop(processed_guard);
        }
        
        // Fast remove: Batches ƒë√£ processed
        if !to_remove.is_empty() {
            let mut missed_guard = self.missed_batches.lock().await;
            for batch_digest in to_remove {
                missed_guard.remove(&batch_digest);
            }
        }
        
        // Fast log: Missed batches v√† update retry_count
        if !to_log.is_empty() {
            let mut missed_guard = self.missed_batches.lock().await;
            for (batch_digest, consensus_index, round, block_height) in to_log {
                if let Some(info) = missed_guard.get(&batch_digest) {
                    let elapsed = now.duration_since(info.commit_time);
                    warn!("‚ö†Ô∏è [UDS] Missed batch detected: BatchDigest={:?}, ConsensusIndex={}, Round={}, BlockHeight={}, Elapsed={:?}ms", 
                        batch_digest, consensus_index, round, block_height, elapsed.as_millis());
                    // Update retry_count ƒë·ªÉ ch·ªâ log m·ªôt l·∫ßn
                    if let Some(missed_info) = missed_guard.get_mut(&batch_digest) {
                        missed_info.retry_count = 1;
                    }
                }
            }
        }
        
        // T·ªêI ∆ØU: Lazy GC - ch·ªâ khi cache qu√° l·ªõn
        {
            let mut missed_guard = self.missed_batches.lock().await;
            const MAX_MISSED_BATCHES: usize = 5000;
            if missed_guard.len() > MAX_MISSED_BATCHES {
                // Fast GC: S·ª≠ d·ª•ng retain thay v√¨ sort ƒë·ªÉ tr√°nh allocation
                let target_size = MAX_MISSED_BATCHES / 2;
                let current_size = missed_guard.len();
                if current_size > target_size {
                    // Remove entries c≈© nh·∫•t (kh√¥ng sort - ch·ªâ remove random entries ƒë·ªÉ tr√°nh overhead)
                    let mut removed = 0;
                    missed_guard.retain(|_, _| {
                        removed += 1;
                        removed <= target_size || (removed - target_size) % 2 == 0
                    });
                    debug!("üßπ [UDS] GC: Cleaned missed_batches from {} to {} entries", current_size, missed_guard.len());
                }
            }
        }
    }
}

impl UdsExecutionState {
    /// G·ª≠i block cho m·ªôt block height c·ª• th·ªÉ
    async fn send_block_for_height(&self, block_height: u64) {
        let mut current_block_guard = self.current_block.lock().await;
        
        if let Some(block) = current_block_guard.take() {
            if block.height == block_height {
                // Block ƒë√∫ng height ‚Üí g·ª≠i
                drop(current_block_guard);
                let (block_to_send, tx_hash_map, batch_digests) = block.finalize();
                
                // Atomic check-and-send
                let mut last_sent_guard = self.last_sent_height.lock().await;
                let should_send = last_sent_guard.is_none() || 
                    block_to_send.height > last_sent_guard.unwrap();
                
                if should_send {
                    // Fill gaps tr∆∞·ªõc khi g·ª≠i block
                    if let Some(last_sent_val) = *last_sent_guard {
                        if block_to_send.height > last_sent_val + 1 {
                            drop(last_sent_guard);
                            if let Err(e) = self.send_empty_blocks_for_gaps(last_sent_val + 1, block_to_send.height).await {
                                error!("‚ùå [UDS] Failed to send empty blocks for gaps: {}", e);
                            }
                            last_sent_guard = self.last_sent_height.lock().await;
                            // Re-check sau khi fill gaps
                            if let Some(updated_last_sent) = *last_sent_guard {
                                if block_to_send.height <= updated_last_sent {
                                    drop(last_sent_guard);
                                    debug!("‚è≠Ô∏è [UDS] Block {} already sent during gap filling", block_to_send.height);
                                    return;
                                }
                            }
                        }
                    } else {
                        // Ch∆∞a g·ª≠i block n√†o ‚Üí fill t·ª´ 0
                        if block_to_send.height > 0 {
                            drop(last_sent_guard);
                            if let Err(e) = self.send_empty_blocks_for_gaps(0, block_to_send.height).await {
                                error!("‚ùå [UDS] Failed to send empty blocks for gaps: {}", e);
                            }
                            last_sent_guard = self.last_sent_height.lock().await;
                            // Re-check sau khi fill gaps
                            if let Some(updated_last_sent) = *last_sent_guard {
                                if block_to_send.height <= updated_last_sent {
                                    drop(last_sent_guard);
                                    debug!("‚è≠Ô∏è [UDS] Block {} already sent during gap filling", block_to_send.height);
                                    return;
                                }
                            }
                        }
                    }
                    
                    // Final check
                    let final_should_send = last_sent_guard.is_none() || 
                        block_to_send.height > last_sent_guard.unwrap();
                    
                    if final_should_send {
                        drop(last_sent_guard);
                        
                        if !block_to_send.transactions.is_empty() {
                            info!("üì§ [UDS] Sending block {} (consensus_index range {}): Epoch={}, TxCount={}", 
                                block_to_send.height, 
                                format!("{}-{}", block_to_send.height * BLOCK_SIZE, (block_to_send.height + 1) * BLOCK_SIZE - 1),
                                block_to_send.epoch, 
                                block_to_send.transactions.len());
                        }
                        
                        // Clone tx_hash_map v√† batch_digests ƒë·ªÉ d√πng sau khi send_block_with_retry
                        let tx_hash_map_clone = tx_hash_map.clone();
                        if let Err(e) = self.send_block_with_retry(block_to_send.clone(), tx_hash_map, batch_digests).await {
                            error!("‚ùå [UDS] Failed to send block height {} after retries: {}", block_to_send.height, e);
                        } else {
                            // Atomic update
                            let mut last_sent_guard = self.last_sent_height.lock().await;
                            let should_update = last_sent_guard.is_none() || 
                                block_to_send.height > last_sent_guard.unwrap();
                            if should_update {
                                *last_sent_guard = Some(block_to_send.height);
                                
                                if !block_to_send.transactions.is_empty() {
                                    info!("‚úÖ [UDS] Block {} sent successfully with {} transactions", 
                                        block_to_send.height, block_to_send.transactions.len());
                                    
                                    // Log transaction hashes ƒë·ªÉ trace (d√πng hash ƒë√£ l∆∞u s·∫µn - ƒë·∫£m b·∫£o nh·∫•t qu√°n)
                                    for (idx, tx) in block_to_send.transactions.iter().enumerate() {
                                        let digest_key = tx.digest.as_ref().to_vec();
                                        if let Some(tx_hash_hex) = tx_hash_map_clone.get(&digest_key) {
                                            info!("  ‚úÖ [UDS] Block {} Tx[{}] executed: TxHash={}, WorkerId={}", 
                                                block_to_send.height, idx, tx_hash_hex, tx.worker_id);
                                        } else {
                                            error!("  ‚ùå [UDS] Block {} Tx[{}] executed but hash not found in map: WorkerId={}", 
                                                block_to_send.height, idx, tx.worker_id);
                                        }
                                    }
                                }
                            }
                        }
                    }
                } else {
                    // Block ƒë√£ ƒë∆∞·ª£c g·ª≠i b·ªüi concurrent call
                    debug!("‚è≠Ô∏è [UDS] Block {} already sent (skipping duplicate)", block_to_send.height);
                }
            } else {
                // Block height kh√°c ‚Üí ƒë·∫∑t l·∫°i v√†o current_block
                *current_block_guard = Some(block);
            }
        }
    }
    
    /// G·ª≠i empty block cho m·ªôt height c·ª• th·ªÉ
    async fn send_empty_block(&self, height: u64) -> Result<(), String> {
        let empty_block = comm::CommittedBlock {
            epoch: self.epoch,
            height,
            transactions: Vec::new(),
        };
        
        // Atomic check-and-send
        let last_sent_guard = self.last_sent_height.lock().await;
        let should_send = last_sent_guard.is_none() || height > last_sent_guard.unwrap();
        
        if should_send {
            drop(last_sent_guard);
            let empty_tx_hash_map = HashMap::new();
            let empty_batch_digests = Vec::new();
            self.send_block_with_retry(empty_block, empty_tx_hash_map, empty_batch_digests).await.map_err(|e| format!("{}", e))?;
            
            let mut last_sent_guard = self.last_sent_height.lock().await;
            let should_update = last_sent_guard.is_none() || height > last_sent_guard.unwrap();
            if should_update {
                *last_sent_guard = Some(height);
            }
        }
        Ok(())
    }

    async fn load_execution_indices(&self) -> ExecutionIndices {
        let last_consensus_index = {
            let guard = self.last_consensus_index.lock().await;
            *guard
        };
        
        ExecutionIndices {
            next_certificate_index: last_consensus_index + 1,
            next_batch_index: 0,
            next_transaction_index: 0,
        }
    }
}

/// Simple execution state for testing/fallback (sends transactions to channel)
pub struct SimpleExecutionState {
    tx_confirmation: tokio::sync::mpsc::Sender<u64>,
}

impl SimpleExecutionState {
    pub fn new(tx_confirmation: tokio::sync::mpsc::Sender<u64>) -> Self {
        Self { tx_confirmation }
    }
}

#[async_trait]
impl ExecutionState for SimpleExecutionState {
    async fn handle_consensus_transaction(
        &self,
        _consensus_output: &ConsensusOutput,
        _execution_indices: ExecutionIndices,
        transaction: Vec<u8>,
    ) {
        // Deserialize transaction as u64 for testing
        if let Ok(value) = bincode::deserialize::<u64>(&transaction) {
            let _ = self.tx_confirmation.send(value).await;
        }
    }

    async fn load_execution_indices(&self) -> ExecutionIndices {
        // SimpleExecutionState doesn't track state, return default
        ExecutionIndices::default()
    }
}
