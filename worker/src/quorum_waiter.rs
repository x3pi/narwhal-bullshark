// Copyright (c) 2021, Facebook, Inc. and its affiliates
// Copyright (c) 2022, Mysten Labs, Inc.
// SPDX-License-Identifier: Apache-2.0
use config::{Committee, SharedWorkerCache, Stake, WorkerId};
use crypto::PublicKey;
use fastcrypto::hash::Hash;
use futures::stream::{futures_unordered::FuturesUnordered, StreamExt as _};
use network::{CancelOnDropHandler, P2pNetwork, ReliableNetwork};
use std::{pin::Pin, time::Duration};
use tokio::{sync::watch, task::JoinHandle, time::{sleep, Instant, Sleep}};
use tracing::{debug, info, warn, error};
use types::{
    error::DagError,
    metered_channel::{Receiver, Sender},
    Batch, ReconfigureNotification, WorkerMessage,
};

#[cfg(test)]
#[path = "tests/quorum_waiter_tests.rs"]
pub mod quorum_waiter_tests;

/// The QuorumWaiter waits for 2f authorities to acknowledge reception of a batch.
pub struct QuorumWaiter {
    /// The public key of this authority.
    name: PublicKey,
    /// The id of this worker.
    id: WorkerId,
    /// The committee information.
    committee: Committee,
    /// The worker information cache.
    worker_cache: SharedWorkerCache,
    /// Receive reconfiguration updates.
    rx_reconfigure: watch::Receiver<ReconfigureNotification>,
    /// Input Channel to receive commands.
    rx_message: Receiver<Batch>,
    /// Channel to deliver batches for which we have enough acknowledgments.
    tx_batch: Sender<Batch>,
    /// A network sender to broadcast the batches to the other workers.
    network: P2pNetwork,
}

impl QuorumWaiter {
    /// Spawn a new QuorumWaiter.
    #[must_use]
    pub fn spawn(
        name: PublicKey,
        id: WorkerId,
        committee: Committee,
        worker_cache: SharedWorkerCache,
        rx_reconfigure: watch::Receiver<ReconfigureNotification>,
        rx_message: Receiver<Batch>,
        tx_batch: Sender<Batch>,
        network: P2pNetwork,
    ) -> JoinHandle<()> {
        tokio::spawn(async move {
            Self {
                name,
                id,
                committee,
                worker_cache,
                rx_reconfigure,
                rx_message,
                tx_batch,
                network,
            }
            .run()
            .await;
        })
    }

    /// Helper function. It waits for a future to complete and then delivers a value.
    async fn waiter(
        wait_for: CancelOnDropHandler<anemo::Result<anemo::Response<()>>>,
        deliver: Stake,
    ) -> Stake {
        let _ = wait_for.await;
        deliver
    }

    /// Main loop.
    async fn run(&mut self) {
        loop {
            tokio::select! {
                Some(batch) = self.rx_message.recv() => {
                    // Broadcast the batch to the other workers.
                    let workers: Vec<_> = self
                        .worker_cache
                        .load()
                        .others_workers(&self.name, &self.id)
                        .into_iter()
                        .map(|(name, info)| (name, info.name))
                        .collect();
                    let (primary_names, worker_names): (Vec<_>, _) = workers.into_iter().unzip();
                    let total_workers = primary_names.len(); // Calculate before moving
                    let message = WorkerMessage::Batch(batch.clone());
                    let handlers = self.network.broadcast(worker_names, &message).await;

                    // Collect all the handlers to receive acknowledgements.
                    let mut wait_for_quorum: FuturesUnordered<_> = primary_names
                        .into_iter()
                        .zip(handlers.into_iter())
                        .map(|(name, handler)| {
                            let stake = self.committee.stake(&name);
                            Self::waiter(handler, stake)
                        })
                        .collect();

                    // Wait for the first 2f nodes to send back an Ack. Then we consider the batch
                    // delivered and we send its digest to the primary (that will include it into
                    // the dag). This should reduce the amount of synching.
                    let threshold = self.committee.quorum_threshold();
                    let mut total_stake = self.committee.stake(&self.name);
                    let batch_digest = batch.digest();
                    let started = Instant::now();
                    
                    // Sync retry timeout: N·∫øu quorum ch∆∞a ƒë·∫°t sau 10 gi√¢y, rebroadcast batch ƒë·ªÉ ƒë·ªìng b·ªô
                    let sync_retry_timeout = Duration::from_secs(10);
                    let mut sync_retry_future: Pin<Box<Sleep>> = Box::pin(sleep(sync_retry_timeout));
                    let mut sync_retry_done = false;
                    
                    // Final timeout: N·∫øu v·∫´n ch∆∞a ƒë·∫°t quorum sau 30 gi√¢y, g·ª≠i batch ƒë·∫øn primary anyway
                    let final_timeout = Duration::from_secs(30);
                    let mut final_timeout_future: Pin<Box<Sleep>> = Box::pin(sleep(final_timeout));
                    let mut received_acks = 0;
                    
                    info!(
                        "‚è≥ [QUORUM] Waiting for quorum for batch {}: Threshold={}, CurrentStake={}, TotalWorkers={}",
                        batch_digest, threshold, total_stake, total_workers
                    );
                    
                    loop {
                        tokio::select! {
                            Some(stake) = wait_for_quorum.next() => {
                                received_acks += 1;
                                total_stake += stake;
                                info!(
                                    "‚úÖ [QUORUM] Received ACK for batch {}: ReceivedAcks={}/{}, TotalStake={}, Threshold={}",
                                    batch_digest, received_acks, total_workers, total_stake, threshold
                                );
                                if total_stake >= threshold {
                                    info!(
                                        "‚úÖ [QUORUM] Quorum reached for batch {}: TotalStake={} >= Threshold={}, Elapsed={:?}",
                                        batch_digest, total_stake, threshold, started.elapsed()
                                    );
                                    if self.tx_batch.send(batch).await.is_err() {
                                        tracing::debug!("{}", DagError::ShuttingDown);
                                    }
                                    break;
                                }
                            }

                            _ = sync_retry_future.as_mut() => {
                                // Sync retry timeout: Rebroadcast batch ƒë·ªÉ ƒë·∫£m b·∫£o c√°c workers nh·∫≠n ƒë∆∞·ª£c
                                // ƒêi·ªÅu n√†y gi√∫p ƒë·ªìng b·ªô batch tr∆∞·ªõc khi vote, tƒÉng kh·∫£ nƒÉng ƒë·∫°t quorum
                                if !sync_retry_done && total_stake < threshold {
                                    warn!(
                                        "üîÑ [QUORUM] Sync retry timeout for batch {}: Elapsed={:?}, ReceivedAcks={}/{}, TotalStake={}, Threshold={}. Rebroadcasting batch to ensure all workers receive it.",
                                        batch_digest, started.elapsed(), received_acks, total_workers, total_stake, threshold
                                    );
                                    
                                    // Rebroadcast batch ƒë·∫øn t·∫•t c·∫£ workers ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªìng b·ªô
                                    let workers_for_rebroadcast: Vec<_> = self
                                        .worker_cache
                                        .load()
                                        .others_workers(&self.name, &self.id)
                                        .into_iter()
                                        .map(|(_, info)| info.name)
                                        .collect();
                                    let rebroadcast_count = workers_for_rebroadcast.len();
                                    let message = WorkerMessage::Batch(batch.clone());
                                    let _rebroadcast_handlers = self.network.broadcast(workers_for_rebroadcast, &message).await;
                                    
                                    info!(
                                        "üîÑ [QUORUM] Rebroadcasted batch {} to {} workers. Waiting for additional ACKs...",
                                        batch_digest, rebroadcast_count
                                    );
                                    
                                    sync_retry_done = true;
                                    // Kh√¥ng break, ti·∫øp t·ª•c ch·ªù quorum ho·∫∑c final timeout
                                } else {
                                    sync_retry_done = true;
                                }
                            }

                            _ = final_timeout_future.as_mut() => {
                                // Final timeout: G·ª≠i batch ƒë·∫øn primary anyway ƒë·ªÉ tr√°nh m·∫•t batch
                                // ƒê√¢y l√† fallback cu·ªëi c√πng n·∫øu v·∫´n kh√¥ng ƒë·∫°t quorum sau khi sync
                                warn!(
                                    "‚ö†Ô∏è [QUORUM] Final timeout for batch {}: Elapsed={:?}, ReceivedAcks={}/{}, TotalStake={}, Threshold={}. Sending batch to primary anyway to prevent loss.",
                                    batch_digest, started.elapsed(), received_acks, total_workers, total_stake, threshold
                                );
                                if self.tx_batch.send(batch).await.is_err() {
                                    error!("‚ùå [QUORUM] Failed to send batch {} to primary after final timeout: {}", batch_digest, DagError::ShuttingDown);
                                } else {
                                    warn!(
                                        "‚ö†Ô∏è [QUORUM] Batch {} sent to primary after final timeout (without full quorum)",
                                        batch_digest
                                    );
                                }
                                break;
                            }

                            result = self.rx_reconfigure.changed() => {
                                result.expect("Committee channel dropped");
                                let message = self.rx_reconfigure.borrow().clone();
                                match message {
                                    ReconfigureNotification::NewEpoch(new_committee)
                                        | ReconfigureNotification::UpdateCommittee(new_committee) => {
                                            self.network.cleanup(self.committee.network_diff(&new_committee));
                                            self.committee = new_committee;
                                            warn!(
                                                "‚ö†Ô∏è [QUORUM] Dropping batch {} due to committee update: Elapsed={:?}, ReceivedAcks={}/{}",
                                                batch_digest, started.elapsed(), received_acks, total_workers
                                            );
                                            break; // Don't wait for acknowledgements.
                                    },
                                    ReconfigureNotification::Shutdown => return
                                }
                            }
                        }
                    }
                },

                // Trigger reconfigure.
                result = self.rx_reconfigure.changed() => {
                    result.expect("Committee channel dropped");
                    let message = self.rx_reconfigure.borrow().clone();
                    match message {
                        ReconfigureNotification::NewEpoch(new_committee) => {
                            self.committee = new_committee;
                        },
                        ReconfigureNotification::UpdateCommittee(new_committee) => {
                            self.committee = new_committee;

                        },
                        ReconfigureNotification::Shutdown => return
                    }
                    tracing::debug!("Committee updated to {}", self.committee);
                }
            }
        }
    }
}
